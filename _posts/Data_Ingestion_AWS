---
layout: post
title: Data Ingestion on AWS
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [AWS, Kinesis]
mathjax: true
summary: Data Collection Systems
---

# Data Ingestion Tools on AWS

Options for real-time data streaming solutions to the cloud using different technologies.
Amazon Web Servies (AWS) provides various services to assist in ingesting data, such as:
- Kinesis Data Streams,
- Kinesis Video Streams,
- Kinesis Firehose,
- Managed Kafka (MSK), and
- Message Queue (MQ) for Apache ActiveMQ or RabbitMQ
- Database Migration Service
- Glue


**Understand scenarios where to use each service**
- Kinesis Data Streams
  - Use when you need custom producers and consumers
  - Use cases that requires sub-second processing
  - Use cases that require unlimited bandwith
- Kinesis Firehose
  - Use cases where you want to deliver directly to S3, Redshift, Elasticsearch, or Splunk
  - Use cases where you tolerate latency of 60 seconds, or greater
  - Use cases where you wish to transform your data or convert the data format
- Database Migration Service
  - Use cases when you need to migrate data from one database to another
  - Use cases where you want to migrate a database to a different database engine
- Glue
  - Batch-oriented use cases where you want to perform an Extract, Transform and Load (ETL) process
  - Not fur use cas with streaming use cases

Understand how much throughput and bandwith each ingestion approach has as well as its ability to scale
- Kinesis Data Streams
  - Shards can handle up to 1,000 PUT records per second
  - Can increase the number of shards in a stream without limit
  - Each shard has a capacity of 1 MB per second for input and 2 MB per second output
- Kinesis Firehose
  - Automatically scales to accommodate the throughput of your stream
- Database Migration Service
  - EC2 instances used for the replication instance
  - You need to scale your replication instance to accommodate your throughput
- Glue
  - Runs in a scale-out Apache Spark environment to move data to target system
  - Scales via Data Processing Units (DPUs) for your ETL jobs

Understand how each service remains available and handles faults
- Kinesis Data Streams
  - Synchronously replicates your shard data across 3 Availablity Zones
- Kinesis Firehose
  - Synchronously replicates your data across 3 Availablity Zones
  - For S3 target, Firehose retries for 24 hours, failure persists past 24 hours your data is lost
  - For Redshift you can specify a retry duration from 0 to 7,200 seconds
  - For Elasticsearch you can specify a retry duration from 0 to 7,200 seconds
  - For Splunk you use a retry duration counter. Firehose retries until counter expires, then backs up your data to S3.
  - Retries may couse duplicate records. AWS uses at-least-once semantics for data delivery.
- Database Migration Service
  - Can use multi-AZ for replication that gives you fault tolerance via redundant replication servers
- Glue
  - Retries 3 times before making a marking an error condition
  - Create a CloudWatch alert for failures that triggers an SNS message


Understand how each service incurs cost
- Kinesis Data Streams
  - Pay per shard hour and PUt payload unit
  - Extended data retention and enhanced fanout incur additional costs
- Kinesis Firehose
  - Pay for the volume of data ingested
  - Pay for data conversions
- Database Migration Service
  - Pay for the EC2 compute resources you use when migrating
  - Pay for log storage
  - Data transfer fees
- Glue
  - Pay an hourly rate at a billing per second for both crawlers and ETL jobs
  - Monthly fee for storing and accessing data in your Glue data catalog


Problems with your streaming data or transforming Data when Ingesting
- Data that is out of order
- Data that is duplicated
- Data where we need to change the format, e.g. from CSV DAta to Parquet Data
- Data that needs to be compressed, e.g. from JSON to GZIP

Methods to address these problems
- Choose an ingestion service that has guaranteed ordering
  - Guaranteed ordering:
    - Kinesis Data Streams
    - DynamoDB Streams
- Choose ann ingestion service that addresses your data duplication requirements, i.e. needs to support de-duped derlivery
  - De-duped delivery:
    - DynamoDB Streams - exactly-once
    - Kinesis Data Streams - at-least-once
      - Embed a primary key in data records and remove duplicates later when processing
    - Kinesis Data Firehose - at-least-once
      - Crawl target data with Glue ML FindMatches Transform
- Use conversion and compression feature of ingestion service
  - Kinesis Data Streams:
    - Use Lambda consumer to format or compress
    - Use KCL application to format or compress
  - Kinesis Firehose:
    - Use format conversion feature if data in JSON, e.g. to Parquet
    - Use Lambda transform to preprocess formation conversion feature if data not JSON
    - Use S3 compression (GZIP, Snappy, or ZIP)
    - Use GZIP COPY command option for Redshift compression
  - Lambda:
    - Convert the format of your data, e.g. GZIP to JSON
    - Transform your data, e.g. expand strings into individual columns
    - Filter your data to remove extraneous info
    - Enrich your data, e.g. error correction
  - Database Migration Service
    - Table and schema transformations, e.g. change table, schema, and/or column names


**What is Amahon Kinesis Data Streams?**
Amazon Kinesis Data Streams is a highly scalable and durable managed streaming service. Use cases include aggregating data from financial markets driving real-time trading decisions to analyzing sensor data at a warehouse to determine when a machine requires servicing or breaks.

**Issue**
Unlike some other AWS services, Kinesis does not provide a native auto-scaling solution like DynamoDB On-Demand or EC2 Auto Scaling. Therefore, there is a need for the right number of shards to be calculated for every stream based on the expected number of records and/or the size of the records. This can lead to over/under-provisioning of shards within a stream resulting in higher costs and/or data ingestion being throttled.


**Implications**
Over/under-provisioning
The obvious implication of over-provisioning is overpaying. Under provisioning results in throttled data ingestion which leads to the next point.

**Throttled data ingestion**
The capacity of a stream is determined by the number of shards in the stream. Each shard can handle up to 1000 records/second or 1MiB/second. Attempting to write more than the provisioned capacity will result in throttling.

**Whatâ€™s available out of the box?**
Amazon makes it simple to scale the stream up or down using the UpdateShardCount, MergeShards, and SplitShard APIs. But, in the absence of an intelligent mechanism for scaling the stream, the responsibility of calling the APIs for increasing/decreasing the number of shards is left to the producer(s). 

### Notifications

What are Shards?
Amazon Kinesis Data Streams are made up of shards. A shard represents a sequence of records in a stream. It is also used to represent the base throughput unit:

One shard can:
- Ingest 1 MiB/second or 1,000 records/second.
- Support up to a maximum total data read rate of 2 MiB/second via the GetRecords API.

**Metrics**

Producers
Record Aggregation
- We can decide whether or not the producer is using Kinesis record aggregation. 
This feature is available in the KPL (Kinesis Producer Library) and allows us to group user records into fewer larger aggregated records.
- Kinesis Record Aggregation increases throughput and reduces cost, at the expense of latency.

Consumers
Enhanced Fan-Out
- We can decide whether or not the consmer is using Kinesis Enhanced Fan-out feature. 
It essentially isolates this consumer from the other consumers, so that the fna-out consumers each have a dedicated, 2MB/s/shard bandwith.
Maximum Consumption Speed
- This is the maximum number of records that a single consumer instance (i.e. process) can handle. This assumes that a single shard is consumed
by exactly one consumer process. It impacts the shard count because it can limit the actual throughput on a shard.

- CloudWatch Metrics can be captured at the stream level or the shard level. IncomingBytes, IncomingRecords, or WriteProvisionedThroughputExceeded are metrics of interest, which are all available for streams and shards.

Scaling frequency: By default, the UpdateShardCount API can only be called a certain number of times per rolling 24-hour period. 

Max and Min Shard Counts: During scale out and scale in operations, the number of shards can be increased to a maximum of 2x and a minimum of 0.5x, where x is the current number of open shards in the stream.

Shard Count Increments: When using the UpdateShardCount API, AWS recommends a target Shard count which is a multiple of 25% (e.g., 25%, 50%, 75%, 100%) of the current open shard count. Any target value within shard limits can be specified. However, if a target is specified which is not a multiple of 25%, the scaling action might take longer to complete. 



All articles I read about Data Collection Systems:

[https://comcastsamples.github.io/KinesisShardCalculator/](https://comcastsamples.github.io/KinesisShardCalculator/)
[https://medium.com/slalom-data-analytics/amazon-kinesis-data-streams-auto-scaling-the-number-of-shards-105dc967bed5](https://medium.com/slalom-data-analytics/amazon-kinesis-data-streams-auto-scaling-the-number-of-shards-105dc967bed5)
[https://towardsdatascience.com/delivering-real-time-streaming-data-to-amazon-s3-using-amazon-kinesis-data-firehose-2cda5c4d1efe](https://towardsdatascience.com/delivering-real-time-streaming-data-to-amazon-s3-using-amazon-kinesis-data-firehose-2cda5c4d1efe)
[https://medium.com/swlh/create-kinesis-firehose-data-stream-from-iot-core-to-s3-using-serverless-framework-2af1d0b35500](https://medium.com/swlh/create-kinesis-firehose-data-stream-from-iot-core-to-s3-using-serverless-framework-2af1d0b35500)
[https://medium.com/dataseries/how-to-build-a-simple-data-lake-using-amazon-kinesis-data-firehose-and-amazon-s3-1a6a6887f441](https://medium.com/dataseries/how-to-build-a-simple-data-lake-using-amazon-kinesis-data-firehose-and-amazon-s3-1a6a6887f441)
[https://faun.pub/apache-kafka-vs-apache-kinesis-57a3d585ef78](https://faun.pub/apache-kafka-vs-apache-kinesis-57a3d585ef78)
