---
layout: post
title: Best Practices for ETL Architectures
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Data Engineering, ETL Architecture, WorkFlow Management]
mathjax: true
summary: ETL Architecture
---

# Best Practices for ETL Architecture

### Extract Necessary Data only

Wether you are doing ETL batch processing (see link) or real-time streaming (see link), nearly all ETL pipelines extract and load more information than you will actually need. Too much data flowing through the ETL pipelines can slow things down considerably.

**How can you efficiently extract the data you need for BI and Analytics?**
- Data profiling and cleansing is essential in order to remove duplicate and unnecessary information.
- Sometimes the right answer may also be an ELT (extract, load, transform) architecture. With ELT, your data is loaded into the data warehouse before being transformed as necessary. ELT allows you to do ad-hoc transformations when you run a particular anaylsis, which is much faster than transforming all of the data before it enters the data warehouse.

### Optimize Your ETL Workflow
Some tips for ETL optimization are:
- Avoid the use of "SELECT * .." and "SELECT DISTINCT ..." in SQL queries during the extraction phase.
- Reduce in-memory merges and joins as much as possible
- Schedule ETL jobs to run overnight or outside peak hours to avoid potential conflicts with other sessions and processes.

### Use Logging and Monitoring
When designing your ETL architecture, deciding what information to include in your logs should be one of your first priorieties. This may include:
- The timing of each data extraction
- The length of time for each extraction
- The number of rows inserted, changed, or deleted during each extraction
- The transcripts for any system or validation errors





