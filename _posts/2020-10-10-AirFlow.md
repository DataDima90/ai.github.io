---
layout: post
title: Best Practices for ETL Architectures
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Data Engineering, ETL Architecture, WorkFlow Management]
mathjax: true
summary: ETL Architecture
---

# What is ETL?
**ETL** is a process of data integration (also referred to as data integration layer) that encompasses three steps - Extraction, Transformation and Loading. ETL systems collects large volumes of raw data from multiple sources, process data and loads that data into one data store (e.g. data warehouse) where it can then be later analyzed.

Let's look at each piece of the extract, transform and load process more closely.

### Extract
Extracting data is the act of targeting a data source (e.g. Salesforce, Google AdWords, etc.) and pulling the data form it into a staging area. We can target many different databases of various types for extractions, and we can run each extraction on a schedule so that you get a regular flow of current and accurate data. The staging area acts as a buffer between the data warehouse and the source data. Since data may be coming from multiple different sources, it's likely in various formats, and directly transferring the data to the warehouse may result in corrupted data. The staging area is used for data cleansing and organization.

### Transform
Since data does not necessarily come to you in the form that you want, it usually needs to be transformed. The data cleaning and organization stage is the Transformation step in the ETL process that will take care of preparing the data to make it most useful to you when you access it, i.e. improving data quality and compliance. ETL yields transformed data through these methods:
- Cleaning
- Filtering: Maybe you want to limit the data that you are storing to just a few fields or 
- Sorting: sort it so that all the columns are in a certain order. 
- Joining: Perhaps you want to join several tables together, or
- Deduplication: maybe you have a messy database full of duplicate records that need to be cleaned.
- Splitting
- Summarization

### Load
Finally, once the data has been sorted, clean, validated and prepared, you want to load data somewhere. The most common load target is a data warehouse, where you can keep it for future analysis and tracking trends. Depending upon your business needs, data can be loaded in batches or all at once. The exact nature of the loading will depend upon the data source, ETL tools, and various other factors.

# Best Practices for ETL Architecture

### Extract Necessary Data only

Wether you are doing ETL batch processing (see link) or real-time streaming (see link), nearly all ETL pipelines extract and load more information than you will actually need. Too much data flowing through the ETL pipelines can slow things down considerably.

**How can you efficiently extract the data you need for BI and Analytics?**
- Data profiling and cleansing is essential in order to remove duplicate and unnecessary information.
- Sometimes the right answer may also be an ELT (extract, load, transform) architecture. With ELT, your data is loaded into the data warehouse before being transformed as necessary. ELT allows you to do ad-hoc transformations when you run a particular anaylsis, which is much faster than transforming all of the data before it enters the data warehouse.

### Optimize Your ETL Workflow
Some tips for ETL optimization are:
- Avoid the use of "SELECT * .." and "SELECT DISTINCT ..." in SQL queries during the extraction phase.
- Reduce in-memory merges and joins as much as possible
- Schedule ETL jobs to run overnight or outside peak hours to avoid potential conflicts with other sessions and processes.

### Use Logging and Monitoring
When designing your ETL architecture, deciding what information to include in your logs should be one of your first priorieties. This may include:
- The timing of each data extraction
- The length of time for each extraction
- The number of rows inserted, changed, or deleted during each extraction
- The transcripts for any system or validation errors


### Choose the Right ETL Tool For You
Before you buy an ETL tool, you must make sure what your considering is compatible with your source and target databases, the type of data you're working with, and your preferred programming language.

Choosing the right ETL tool can only be done when you already have an ETL architecture in mind. You should have a clear idea which data sources and targets will be involved, and which business initiatives your data integration pipelines will support.

# Challenges When Building an ETL Architecture
### Performance
ETL and Performance don't always go together, especially when it comes to batch processing. The transform stage, in particular, is often a performance bottleneck that, if implemented inefficiently, can massively slow down your ETL pipelines.

A further challenge during the data extraction process is how your ETL tool handles strucutred and unstructured data. All of those unstructured itmes (e.g. emails, web pages, etc. (can be difficult to extract without the right tool, and you  may have to create a custom solution to assist you in transferring unstructured data if you chose a tool with poor unstructured data capabilities.

Imroving ETL performance should be one of your primary goals when designing an ETL architechture. One of the best things you can do to optimize your finite ETL resources is to use parallel and distributed processing (e.g. Apache Hadoop and MapReduce). Other ETL architecture solutions include loading data incrementally and partitioning large tables into smaller ones.

### Data Security and Privacy
Much of the data in your ETL pipelines may be sensitive, confidential, or personally-identifying information. Regulations such as the EU's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) strictly govern how organizations can handle and manage their consumer data.

Your ETL architecture must carefully preserve the security and confidentiality of sensitive enterprise data at every step.

### Flexibility to Changing Business Requirements
ETL pipelines depend on stability and predictability: the knowledge that source and target databases will remain in the same locations, the same repeated set of transformations enacted on each new batch of data, and so. on. 





