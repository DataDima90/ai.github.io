---
layout: post
title: AWS Glue Overview
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Glue, AWS]
mathjax: true
summary: All we need to know about AWS Glue
---

# Table of Contents

This page gives us an brief introduction to each of the AWS Glue components and the key concepts and features, such as logging and monitoring, and automation, that we should know before authoring ETL jobs. It is divided into following parts:

- [AWS Glue](#gluedefinition)
- [AWS Glue Use Cases](#gluecase)
- [AWS Glue Architecture](#gluearchitecture)
- [AWS Glue Components](#gluecomponent)
  - [AWS Glue ETL](#glueetl)
  - [AWS Glue Data Catalog](#gluedatacatalog)
  - [AWS Glue features and concepts](#gluefeatures)

<a name="gluedefinition"></a>
# AWS Glue

- AWS Glue is a fully managed serverless ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. 
- AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries.
- AWS Glue is serverless, so we don’t need to set up any cluster or infrastructure, and we can scale up and down as we need. AWS Glue charges only for the amount of compute time it needs to finish a specific workload. Instead of juggling between various instance types to match a workload, AWS Glue comes with the capability of choosing between three worker types recommended for various workloads: Standard, G.1X (for memory-intensive workloads) and G.2X (for workloads with ML transforms).
- AWS Glue is designed to work with semi-structured data. It's common to want to convert semi-structured data into relational tables. Conceptually, we are flattening a hierarchical schema to a relational schema. AWS Glue can perform this conversion for us on-the-fly.
- It introduces a component called a dynamic frame, which we can use in our ETL scripts. A dynamic frame is similar to an Apache Spark dataframe, which is a data abstraction used to organize data into rows and columns, except that each record is self-describing so no schema is required initially. With dynamic frames, we get schema flexibility and a set of advanced transformations specifically designed for dynamic frames. We can convert between dynamic frames and Spark dataframes, so that we can take advantage of both AWS Glue and Spark transformations to do the kinds of analysis that we want.
- We can use the AWS Glue console to discover data, transform it, and make it available for search and querying.
- We can also use the AWS Glue API operations to interface with AWS Glue services.
- We can use a familiar development environment to edit, debug, and test our Python or Scala Apache Spark ETL code.

<a name="gluecase"></a>
# AWS Glue Use Cases

- We can use AWS Glue to organize, cleanse, validate, and format data for storage in a data warehouse or data lake.
- We can use AWS Glue when we run serverless queries against our Amazon S3 data lake. 
- We can create event-driven ETL pipelines with AWS Glue.
- We can use AWS Glue to understand our data assets.


<a name="gluearchitecture"></a>
# AWS Glue Architecture

**The following diagram shows the architecture of an AWS Glue environemnt:**

![]({{ site.url }}/assets/img/posts/awsgluearchitecture.png)

**We typically perform the following actions:**
- For data store sources, we define a crawler to populate our AWS Glue Data Catalog with metadata table definitions. We point our crawler at a data store, and the crawler creates table definitions in the Data Catalog. For streaming sources, we manually define Data Catalog tables and specify data stream properties. In addition to table definitions, the AWS Glue Data Catalog contains other metadata that is required to define ETL jobs. We use this metadata when we define a job to transform our data.
- AWS Glue can generate a script to transform our data. Or, we can provide the script in the AWS Glue console or API.
- We can run our job on demand, or we can set it up to start when a specified trigger occurs. The trigger can be a time-based schedule or an event. When our job runs, a script extracts data from our data source, transforms the data, and loads it to our data target. The script runs in an Apache Spark environment in AWS Glue.

**IMPORTANT:** 

- Tables and databases in AWS Glue are objects in the AWS Glue Data Catalog. They contain metadata; They do NOT contain data from a data store. 
- Text-based data, such as CSVs, must be encoded in `UTF-8` for AWS Glue to process it successfully.

<a name="gluecomponent"></a>
# AWS Glue Components

AWS Glue relies on the interaction of several components to create and manage our extract, transfer, and load (ETL) workflow.

**AWS Glue consists of the following components:**
- **AWS Glue ETL** – AWS Glue ETL gives us batch and streaming options to author code and move, transform, and aggregate data from one source to another.
- **AWS Glue Data Catalog** – Data Catalog provides a view of the metadata of all our data along with options to crawl specific data sources.7
- **AWS Glue DataBrew** – DataBrew is a visual data preparation tool that data analysts and data scientists can use to clean and normalize data. We can choose from more than 250 prebuilt transformations to automate data preparation tasks, all without the need to write any code.

<a name="glueetl"></a>
## AWS Glue ETL
- You can use AWS Glue ETL to move, clean, and transform data from one source to another by using its own functions, Apache Spark, and Python. 
- Using the metadata in the Data Catalog, AWS Glue can automatically generate Scala or PySpark (the Python API for Apache Spark) scripts with AWS Glue extensions that we can use and modify to perform various ETL operations. 
- For example, we can extract, clean, and transform raw data, and then store the result in a different repository, where it can be queried and analyzed. Such a script might convert a CSV file into a relational form and save it in Amazon Redshift.

**AWS Glue ETL code can be authored in the following ways:**

- **Python shell** - When the size of the data is very small, we can use Python shell to write Python scripts to manipulate data.
- **Spark** – We can use Scala and Python to author ETL jobs with AWS Glue and Apache Spark.
- **Spark Streaming** – To enrich, aggregate, and combine streaming data, we can use streaming ETL jobs. In AWS Glue, streaming ETL jobs run on the Apache Spark Structured Streaming engine.
- **AWS Glue Studio** – If someone is new to Apache Spark programming or is accustomed to ETL tools with boxes-and-arrows interfaces, get started by using AWS Glue Studio.

**DPUs and worker types**
- AWS Glue uses data processing units (DPUs), which are also known as worker types. DPUs can be allocated based on the data and velocity needs of a specific use case.

**AWS Glue Python or Scala on Apache Spark**

The following table shows the different AWS Glue worker types for the Apache Spark run environment for batch, streaming, and AWS Glue Studio workloads. 

|               | Standard      | G.1X          | G.2X          |
| ------------- | ------------- | ------------- | ------------- |
| vCPU          | 4             | 4             | 4             |
| Memory        | 16 GB         | 16 GB         | 32 GB         |
| Disc space    | 50            | 64            | 128           |
| Memory        | 2             | 1             | 1             |


**Note that with AWS Glue Studio, we can use only G.1X and G.2X worker types.**

**AWS Glue Streaming ETL**
- AWS Glue enables us to perform ETL operations on streaming data using continuously-running jobs. 
- AWS Glue streaming ETL is built on the Apache Spark Structured Streaming engine, and can ingest streams from Amazon Kinesis Data Streams, Apache Kafka, and Amazon Managed Streaming for Apache Kafka (Amazon MSK). 
- Streaming ETL can clean and transform streaming data and load it into Amazon S3 or JDBC data stores. 
- Use Streaming ETL in AWS Glue to process event data like IoT streams, clickstreams, and network logs.
- If we know the schema of the streaming data source, we can specify it in a Data Catalog table. If not, we can enable schema detection in the streaming ETL job. The job then automatically determines the schema from the incoming data.
- The streaming ETL job can use both AWS Glue built-in transforms and transforms that are native to Apache Spark Structured Streaming.

**AWS Glue Jobs**
- Job is the business logic that is required to perform ETL work. It is composed of a transformation script, data sources, and data targets. Job runs are initiated by triggers that can be scheduled or triggered by events.
- The AWS Glue Jobs system provides managed infrastructure to orchestrate our ETL workflow. 
- We can create jobs in AWS Glue that automate the scripts we use to extract, transform, and transfer data to different locations. - Jobs can be scheduled and chained, or they can be triggered by events such as the arrival of new data.

**AWS Glue Dynamic Frame**
- A distributed table that supports nested data such as structures and arrays. 
- Each record is self-describing, designed for schema flexibility with semi-structured data. 
- Each record contains both data and the schema that describes that data. 
- We can use both dynamic frames and Apache Spark DataFrames in our ETL scripts, and convert between them. 
- Dynamic frames provide a set of advanced transformations for data cleaning and ETL.


<a name="gluedatacatalog"></a>
## AWS Glue Data Catalog
- AWS Glue Data Catalog is our persistent metadata store. It is a managed service that lets us store, annotate, and share metadata in the AWS Cloud in the same way we would in an Apache Hive metastore.
- Each AWS account has one AWS Glue Data Catalog per AWS region. 
- It provides a uniform repository where disparate systems can store and find metadata to keep track of data in data silos, and use that metadata to query and transform the data.
- We can use AWS Identity and Access Management (IAM) policies to control access to the data sources managed by the AWS Glue Data Catalog. IAM policies let us clearly and consistently define which users have access to which data, regardless of its location.
- The Data Catalog also provides comprehensive audit and governance capabilities, with schema change tracking and data access controls. We can audit changes to data schemas. This helps ensure that data is not inappropriately modified or inadvertently shared.

**The following are other AWS services and open source projects that use the AWS Glue Data Catalog:**
- AWS Lake Formation
- Amazon Athena
- Amazon Redshift Spectrum
- Amazon EMR
- AWS Glue Data Catalog Client for Apache Hive Metastore

**The AWS Glue Data Catalog consists of the following components:**
- Databases and tables
- Crawlers and classifiers
- Connections
- AWS Glue Schema Registry

### AWS Glue databases and tables
- The Data Catalog consists of database and tables. 
- A table can be in only one database. 
- Our database can contain tables from many different sources that AWS Glue supports.

**We can create the database and tables in the following ways:**
- The AWS Glue crawler
- An AWS Glue ETL job
- AWS Glue console
- The `CreateTable` operation in the AWS Glue API
- AWS CloudFormation templates
- A migrated Apache Hive metastore

**Note:** Currently, AWS Glue supports Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB, and Java Database Connectivity (JDBC) data sources.

### AWS Glue crawlers and classifiers
- A crawler helps create and update the Data Catalog tables. 
- It can crawl both file-based and table-based data stores.

**Crawlers can crawl the following data stores through their respective native interfaces:**
- Amazon S3
- DynamoDB

**Crawlers can crawl the following data stores through a JDBC connection:**
- Amazon Redshift
- Amazon Relational Database Service (Amazon RDS)
  - Amazon Aurora
  - Microsoft SQL Server
  - MySQL
  - Oracle
  - PostgreSQL
- Publicly accessible databases (on-premises or on another cloud provider environment)
  - Aurora
  - Microsoft SQL Server
  - MySQL
  - Oracle
  - PostgreSQL

**Populating the AWS Glue Data Catalog**

- The AWS Glue Data Catalog contains references to data that is used as sources and targets of our extract, transform, and load (ETL) jobs in AWS Glue. To create our data warehouse or data lake, we must catalog this data. 
- The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of our data. 
- We use the information in the Data Catalog to create and monitor our ETL jobs. 
- Information in the Data Catalog is stored as metadata tables, where each table specifies a single data store. Typically, we run a crawler to take inventory of the data in our data stores, but there are other ways to add metadata tables into your Data Catalog. 


**The following workflow diagram shows how AWS Glue crawlers interact with data stores and other elements to populate the Data Catalog:**

![]({{ site.url }}/assets/img/posts/aws_glue_architecture.png)


**The following is the general workflow for how a crawler populates the AWS Glue Data Catalog:**

1. A crawler runs any custom classifiers that we choose to infer the format and schema of our data. We provide the code for custom classifiers, and they run in the order that we specify. The first custom classifier to successfully recognize the structure of our data is used to create a schema. Custom classifiers lower in the list are skipped.
2. If no custom classifier matches our data's schema, built-in classifiers try to recognize our data's schema. An example of a built-in classifier is one that recognizes JSON.
3. The crawler connects to the data store. Some data stores require connection properties for crawler access.
4. The inferred schema is created for our data.
5. The crawler writes metadata to the Data Catalog. A table definition contains metadata about the data in our data store. The table is written to a database, which is a container of tables in the Data Catalog. Attributes of a table include classification, which is a label created by the classifier that inferred the table schema.


### AWS Glue connections
- We can use connections to define connection information, such as login credentials and virtual private cloud (VPC) IDs, in one place. This saves time, because we don’t need to provide connection information every time we create a crawler or job.

**The following connection types are available:**
- JDBC
- Amazon RDS
- Amazon Redshift
- MongoDB, including Amazon DocumentDB (with MongoDB compatibility)
- Network (designates a connection to a data source within a VPC environment on AWS)

### AWS Glue Schema Registry
- The AWS Glue Schema Registry enables disparate systems to share a schema for serialization and deserialization. 

**With the AWS Glue Schema Registry, we can manage and enforce schemas on our data streaming applications using convenient integrations with the following data input sources:**
- Apache Kafka
- Amazon Managed Streaming for Apache Kafka
- Amazon Kinesis Data Streams
- Amazon Kinesis Data Analytics for Apache Flink
- AWS Lambda

**Schema Registry consists of the following components:**
- **Schemas** – A schema is the abstraction to represent the structure and format of a data record.
- **Registry** – A registry is a logical container of schemas. Registries allow you to organize your schemas and manage access control for your applications.

<a name="gluefeatures"></a>
## AWS Glue Important features and concepts
- With AWS Glue, we can debug, monitor performance, and automate and run jobs in the same S3 bucket without re-running a job on the same set of data.

### Logging and monitoring
- By default, AWS Glue sends logs to Amazon CloudWatch under the `aws-glue` log group.

**In addition to the default logs, AWS Glue provides the following options for more advanced level monitoring:**
- **Job metrics** – After we turn on Job metrics, metrics are reported every 30 seconds to the Glue namespace.
- **Continuous logging** – AWS Glue provides real-time, continuous logging for AWS Glue jobs. We can view real-time Apache Spark job logs in CloudWatch, including driver logs, executor logs, and an Apache Spark job progress bar.
- **Spark UI** – When our AWS Glue ETL job runs using Spark, we can store Spark UI logs. We can then deploy the Spark history server to view them on an Amazon Elastic Compute Cloud (Amazon EC2) instance. We could also view them locally using Docker. 

**Note:** that Monitoring options are not available for Python shell jobs.

### Automation
- AWS Glue provides two ways for us to automate ETL jobs: triggers and workflows.
- **AWS Glue triggers** - We can design a basic chain of dependent jobs and crawlers based on a specific schedule or condition by using AWS Glue triggers. 
- **AWS Glue workflows** - For more complex workloads, we can use workflows to create and visualize complex ETL activities that involve multiple crawlers, jobs, and triggers. Each workflow manages running and monitoring all its components.
- **Integration with other AWS services** - AWS Glue also integrates with other AWS services, such as Lambda and AWS Step Functions, for more automation options.

### Job bookmarks
- To avoid reprocessing data when a scheduled ETL job runs, AWS Glue uses job bookmarks.
- AWS Glue tracks data that has already been processed during a previous run of an ETL job by persisting state information from the job run.
- The persisted state information is the job bookmark.
- Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data.



# AWS Glue
- Glue is a fully managed serverless ETL service that automates the time-consuming steps of data preparation for analytics.
- Glue combines the speed and power of Apache Spark with the lightweight data organization of Hive metastores to GLUE together disparate data sources from across AWS
- Glue components help automate much of the undifferentiated heavy lifting involved with discovering, categorizing, cleaning, enriching, and moving data, so more time can be spent on analyzing the data.
- Glue automatically discovers and profiles the data via the Glue Data Catalog, recommends and generates ETL code to transform the source data into target schemas, and runs the ETL jobs on a fully managed, scale-out Apache Spark environment to load the data into its destination.
- Glue also helps setup, orchestrate, and monitor complex data flows.
- Glue consists of a 
  - Data Catalog, which is a central metadata repository, 
  - ETL engine that can automatically generate Scala or Python code,
  - Flexible scheduler that handles dependency resolution, job monitoring, and retries.
- Glue makes it simple and cost-effective to catalog, clean, enrich, and move data reliably between various data stores and streams.
- Glue is serverless and supports pay-as-you-go model. There is no infrastructure to provision or manage. 
- Glue supports data stored in
  - RDS (Aurora, MySQL, Oracle, PostgreSQL, SQL Server)
  - Redshift
  - DynamoDB
  - S3
  - JDBC databases in the Virtual Private Cloud (VPC) running on EC2
- Glue also supports custom Scala or Python code and import custom libraries and Jar files into the AWS Glue ETL jobs to access data sources not natively supported by AWS Glue.
- Glue provides development endpoints to edit, debug, and test the code it generates.
- Glue provides a unified view of the data via the Glue Data Catalog that is available for ETL, querying and reporting using services like Athena, EMR, and Redshift Spectrum.
- Glue can automatically discover both structured and semi-structured data stored in the data lake on S3, data warehouse in Redshift, and various databases running on AWS.
- Glue strives to address both data setup and processing in one place with minimal infrastructure setup. 
- - The Glue data catalog can make both file-based and traditional data sources available to Glue jobs, including schema detection through crawlers. 
- Glue’s data catalog can share a Hive metastore with AWS Athena, a convenient feature for existing Athena users like us.

![]({{ site.url }}/assets/img/posts/aws_glue_architecture.png)

**Usage patterns**
- Crawl your data and generate code to execute, including data transformation and loading
- Integrate with services like Athena, EMR, and Redshift
- Generates customizable, reusable, and portable ETL code using Python and Spark
- Glue's default timeout is two days and a good fit for jobs that were too long or too unpredictable - unlike Lambda's max of 15 minutes.
- Glue ETL supports serverless streaming ETL
  - Consumes from Kinesis or Managed Streaming for Kafka (MSK)
  - Clean & Transform in flight
  - Store results into S3 or other data stores 

![]({{ site.url }}/assets/img/posts/glueusagepattern.png)

**Cost**
- Hourly rate, billed by the minute, for crawler and ETL jobs
- Glue Data Catalog: pay a monthly fee for storing and accessing the metadata
  - First million objects stored and accesess are free for the Glue Data Catalog
- Development endpoints for developing ETL code charged by the minute

**Performance**
- Scale-out Apache Spark environment to load data to target data store
- Specify the number of Data Processing Units (DPUs) to allocate to your ETL job. 

**Durability and availability**
- Glue leverages the durability of the data stores to which you connect
- Provides job status and pushes notifications to CloudWatch events
- Use SNS notifications from CloudWatch events to notify of job failures or success

**Scalability and elasticity**
- Runs on top of the Apache Spark for transformation job scale-out execution

**Interfaces**
- Crawlers scan many data store types
- Bulk import Hive metastore into Glue Data Catalog

**Anti-patterns**
- Streaming data, unless Spark Streaming
- Heterogeneous ETL job types, i.e. if you want to use other engines (Hive, Pig, etc.) Data Pipeline, then EMR would be a better fit
- NoSQL databases: not supported as data source

**Security**
- IAM policies for the Glue service
- Configure Glue to only access JDBC through SSL
- Data Catalog: Encrypted by KMS
- Connection passwords: Encrypted by KMS
- Server-side Encryption (SSE) for data at rest
- SSL for data (in transit/motion)
- Data written by AWS Glue - Security Configurations:
  - S3 encryption mode: SSE-S3 or SSE-KMS
  - CloudWatch encryption mode
  - Job bookmark encryption mode 

**Monitoring**
- Fire off Lambda function or SNS for notification when ETL succeeds or fails
- Glue produces metrics for crawlers and jobs for monitoring
- Statistics are about the health of your environment and are written to the Glue Data Catalog
- Use automated monitoring tools to watch Glue and report problems
  - CloudWatch events
  - CloudWatch logs
  - CloudTrail logs


# AWS Glue Data Catalog

![]({{ site.url }}/assets/img/posts/glue_architecture.png)

- Glue Data Catalog is a central repository and persistent metadata store to store structural and operational metadata for all the data assets
- Glue Data Catalog provides a uniform repository where disparate systems can store and find metadata to keep track of data in data silos, and use that metadata to query and transform the data
- Table definitions once added to the Glue Data Catalog, are available for ETL and also readily available for querying in Athena, EMR, and Redshift Spectrum to provide a common view of the data between these services.
- Glue Data Catalog can serve as a Hive "metastore" and can also import a Hive metastore into Glue. Reminder: Hive lets you run SQL-like queries from EMR



# AWS Glue Crawler

- Glue Crawler connects to a data store, processes through a prioritized list of classifiers to extract the schema of the data and other statistics, and then populates the Glue Data Catalog with this metadata
- Glue Crawler scans various data stores to automatically infer schemas and partition structure to populate the Glue Data Catalog with corresponding table definitions and statistics.
- Glue Crawler can be scheduled to run periodically so that the metadata is always up-to-date and in-sync with the underlying data
- Glue Crawler automatically add new tables, new partitions to existing table, and new versions of table definitions
- Glue Crawler will extract paritions based on how your S3 data is organized.

## Dynamic Frames

- AWS Glue is designed to work with semi-structured data and introduces a component called a dynamic frame, which you can use in the ETL script
- Dynamic frame is a distributed table that supports nested data such as structures and arrays
- Each record is self-describing, designed for schema flexibility with semi-structured data. Each record contains both data and the schema that describes that data.
- A Dynamic Frame is similar to an Apache Spark dataframe, which is a data abstraction used to organize data into rows and columns, except that each record is self-describing so no schema is required initially.
- Dynamic frames provide schema flexibility and a set of advanced transformations specifically designed for dynamic frames.
- Conversion can be done between Dynamic frames and Spark dataframes, to take advantage of both AWS Glue and Spark transformations to do the kinds of analysis needed.

# Glue ETL on Apache Spark

- Use Glue ETL when you do not need or want to pay for an EMR cluster
- Fully managed, cost effective, pay only for the resources consumed
- Jobs runs on a serverless Spark platform
- Glue ETL jobs use the concept of concurrent data processing units (DPUs) to describe the compute capacity of a job run. A single DPU provides four virtual central processing units (vCPUs) and 16 GB of memory.

## Glue ETL Jobs - Structure
A Glue job defines the business logic that performs the ETL work in AWS Glue

- Glue runs your script to extract data from your sources, transform it, clean it, enrich it and load it into your targets
- Glue triggers can start jobs based on a schedule or event, or on demand.
- Monitor your job runs to get runtime metrics: completion status, duration, etc.
- Glue ETL generates automatically code in Python or Scala, which you can modify
- Glue ETL provides your own Spark or PySpark scripts

## Glue ETL Jobs - Types
Glue output file formats JSON, CSV, ORC (Optimized Row Columnar), Apache Parquet, and Apache Avro
Three types of Glue jobs:
- Spark ETL job: executed in managed Apache Spark environment, processes data in batches
- Streaming ETL job: uses the Apache Spark Structured Streaming framework
- Python shell job: schedule and run tasks that do not require an Apache Spark environment

## Glue ETL Jobs - Transformations
Glue has built-in transforms for processing data
- Call from within your ETL script
- In a DynamicFrame (an extension of an Apache Spark SQL DataFrame), your data passes from transfrom to transform
- Built-in transform types (subset)
  - ApplyMapping: maps source DynamicFrame columns and data types to target DynamicFrame columns and data types
  - Filter: selects records from a DynamicFrame and returns a filtered DynamicFrame
  - Map: applies a function to the records of a DynamicFrame and returns a transformed DynamicFrame
  - Relationalize: converts a DynamicFrame to a relational (rows and columns) form
- Machine Learning Transformations
  - FindMatches ML: identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly
- Format conversions: CSV, JSON, Avro, Parquet, ORC, XML
- Apache Spark transformations, e.g. K-Means

## Glue ETL jobs - Triggers
A trigger can start specificed jobs and crawlers
- On demand, based on a schedule, or based on combination of events
- Add a trigger via the Glue console, the CLI or the Glue API
- Activate or deactivate a trigger via the Glue console, the CLI, or the Glue API

Profile your Glue jobs using metrics and visualize on the Glue and CloudWatch consoles to identify and fix issues

## AWS Glue Streaming ETL

- AWS Glue enables performing ETL operations on streaming data using continuously-running jobs.
- AWS Glue streaming ETL is built on the Apache Spark Structured Streaming engine, and can ingest streams from Kinesis Data Streams and Apache Kafka using Amazon Managed Streaming for Apache Kafka.
- Streaming ETL can clean and transform streaming data and load it into S3 or JDBC data stores.
- Use Streaming ETL in AWS Glue to process event data like IoT streams, clickstreams, and network logs.

## Glue Job Bookmarks
- Persists state from the job run
- Prevents reprocessing of old data
- Allows you to proces new data only when re-running on schedule
- Works with S3 sources in a variety of formats
- Works with JDBC (if PK's are in sequential order)
  - IMPORTANT: Only handles new ros, NOT updated rows 

# AWS Glue Studio
- Visual interface for ETL workflows
- Visual job editor
  - Create DAG's for complex workflows
  - Data Sources include S3, Kinesis, Kafka, JDBC
  - Allows Transform, sample and join data
  - Target to S3 or Glue Data Catalog
  - Support partitioning
- Visual job dashboard

# AWS Glue DataBrew
- A visual data preparation tool to clean and normalize data
  - UI for pre-processing large data sets
  - Input from S3, data warehouse, or database
  - Output to S3
- Over 250 read-made transformations avaiable
- You create steps of transformations that can be saved as jobs within a larger project
- Security:
  - KMS
  - SSL in transit
  - IAM can restrict who can do what
  - CloudWatch & CloudTrail


# Sources
- [https://medium.com/capital-one-tech/aws-glue-an-etl-solution-with-huge-potential-91a04a2a0712](https://medium.com/capital-one-tech/aws-glue-an-etl-solution-with-huge-potential-91a04a2a0712)
- [https://medium.com/@Synerzip/a-practical-guide-to-aws-glue-7bc3f86c2820](https://medium.com/@Synerzip/a-practical-guide-to-aws-glue-7bc3f86c2820)
- [https://towardsdatascience.com/aws-glue-101-all-you-need-to-know-with-a-real-world-example-f34af17b782f](https://towardsdatascience.com/aws-glue-101-all-you-need-to-know-with-a-real-world-example-f34af17b782f)
