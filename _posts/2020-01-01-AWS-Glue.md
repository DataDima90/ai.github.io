---
layout: post
title: AWS Glue
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Glue, AWS]
mathjax: true
summary: AWS Glue Overview
---

# AWS Glue

- A fully managed ETL service that automates the time-consuming steps of data preparation for analytics.
- Makes it simple and cost-effective to catalog, clean, enrich, and move data reliably between various data stores and streams.
- Is serverless and supports pay-as-you-go model. There is no infrastructure to provision or manage. 


**Usage patterns**
- Crawl your data and generate code to execute, including data transformation and loading
- Integrate with services like Athena, EMR, and Redshift
- Generates customizable, reusable, and portable ETL code using Python and Spark

![]({{ site.url }}/assets/img/posts/glueusagepattern.png)

**Cost**
- Hourly rate, billed by the minute, for crawler and ETL jobs
- Glue Data Catalog: pay a monthly fee for storing and accessing the metadata

**Performance**
- Scale-out Apache Spark environment to load data to target data store
- Specify the number of Data Processing Units (DPUs) to allocate to your ETL job

Connect to many data sources, S3, RDS, or many other types of data sources

**Durability and availability**
- Glue leverages the durability of the data stores to which you connect
- Provides job status and pushes notifications to CloudWatch events
- Use SNS notifications from CloudWatch events to notify of job failures or success

**Scalability and elasticity**
- Runs on top of the Apache Spark for transformation job scale-out execution

**Interfaces**
- Crawlers scan many data store types
- Bulk import Hive metastore into Glue Data Catalog

**Anti-patterns**
- Streaming data, unless Spark Streaming
- Heterogeneous ETL job types; use EMR
- NoSQL databases: not supported as data source


# Glue ETL on Apache Spark

- Use Glue when you do not need or want to pay for an EMR cluster
- Glue generates an Apache Spark (PySpark or Scala) script
- Glue runs in a fully managed Apache Spark environment

- Spark has 4 primary libraries:
  - Spark SQL
  - Spark Streaming
  - MLlib
  - GraphX

## Glue ETL Jobs - Structure
A Glue job defines the business logic that performs the ETL work in AWS Glue
- Glue runs your script to extract data from your sources, transform the data, and load it into your targets
- Glue triggers can start jobs based on a schedule or event, or on demand.
- Monitor your job runs to get runtime metrics: completion status, duration, etc.
- Based on your source schema and target location or schema, the Glue code generator automatically creates an Apache Spark API (PySpark) script.
  - Edit the script to customize to your requirements.

## Glue ETL Jobs - Types
Glue output file formats JSON, CSV, ORC (Optimized Row Columnar), Apache Parquet, and Apache Avro
Three types of Glue jobs:
- Spark ETL job: executed in managed Apache Spark environment, processes data in batches
- Streaming ETL job: uses the Apache Spark Structured Streaming framework
- Python shell job: schedule and run tasks that do not require an Apache Spark environment

## Glue ETL Jobs - Transforms
Glue has built-in transforms for processing data
- Call from within your ETL script
- In a DynamicFrame (an extension of an Apache Spark SQL DataFrame), your data passes from transfrom to transform
- Built-in transform types (subset)
  - ApplyMapping: maps source DynamicFrame columns and data types to target DynamicFrame columns and data types
  - Filter: selects records from a DynamicFrame and returns a filtered DynamicFrame
  - Map: applies a function to the records of a DynamicFrame and returns a transformed DynamicFrame
  - Relationalize: converts a DynamicFrame to a relational (rows and columns) form

## Glue ETL jobs - Triggers
A trigger can start specificed jobs and crawlers
- On demand, based on a schedule, or based on combination of events
- Add a trigger via the Glue console, the CLI or the Glue API
- Activate or deactivate a trigger via the Glue console, the CLI, or the Glue API

## Glue ETL jobs - Monitoring
Glue produces metrics for crawlers and jobs for monitoring
- Statistics about the health of your environment
- Statistics are written to the Glue Data Catalog

Use automated monitoring tools to watch Glue and report problems
- CloudWatch events
- CloudWatch logs
- CloudTrail logs

Profile your Glue jobs using metrics and visualize on the Glue and CloudWatch consoles to identify and fix issues

