---
layout: post
title: AWS Glue
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Glue, AWS]
mathjax: true
summary: AWS Glue Overview
---

# AWS Glue

- Glue is a fully managed ETL service that automates the time-consuming steps of data preparation for analytics.
- Glue components help automate much of the undifferentiated heavy lifting involved with discovering, categorizing, cleaning, enriching, and moving data, so more time can be spent on analyzing the data.
- Glue automatically discovers and profiles the data via the Glue Data Catalog, recommends and generates ETL code to transform the source data into target schemas, and runs the ETL jobs on a fully managed, scale-out Apache Spark environment to load the data into its destination.
- Glue also helps setup, orchestrate, and monitor complex data flows.
- Glue consists of a 
  - Data Catalog, which is a central metadata repository, 
  - ETL engine that can automatically generate Scala or Python code,
  - Flexible scheduler that handles dependency resolution, job monitoring, and retries.
- Glue makes it simple and cost-effective to catalog, clean, enrich, and move data reliably between various data stores and streams.
- Glue is serverless and supports pay-as-you-go model. There is no infrastructure to provision or manage. 
- Glue supports data stored in
  - RDS (Aurora, MySQL, Oracle, PostgreSQL, SQL Server)
  - Redshift
  - DynamoDB
  - S3
  - JDBC databases in the Virtual Private Cloud (VPC) running on EC2
- Glue also supports custom Scala or Python code and import custom libraries and Jar files into the AWS Glue ETL jobs to access data sources not natively supported by AWS Glue.
- Glue provides development endpoints to edit, debug, and test the code it generates.
- Glue provides a unified view of the data via the Glue Data Catalog that is available for ETL, querying and reporting using services like Athena, EMR, and Redshift Spectrum.
- Glue can automatically discover both structured and semi-structured data stored in the data lake on S3, data warehouse in Redshift, and various databases running on AWS.

![]({{ site.url }}/assets/img/posts/aws_glue_architecture.png)

**Usage patterns**
- Crawl your data and generate code to execute, including data transformation and loading
- Integrate with services like Athena, EMR, and Redshift
- Generates customizable, reusable, and portable ETL code using Python and Spark
- Glue ETL supports serverless streaming ETL
  - Consumes from Kinesis or Managed Streaming for Kafka (MSK)
  - Clean & Transform in flight
  - Store results into S3 or other data stores 

![]({{ site.url }}/assets/img/posts/glueusagepattern.png)

**Cost**
- Hourly rate, billed by the minute, for crawler and ETL jobs
- Glue Data Catalog: pay a monthly fee for storing and accessing the metadata
  - First million objects stored and accesess are free for the Glue Data Catalog
- Development endpoints for developing ETL code charged by the minute

**Performance**
- Scale-out Apache Spark environment to load data to target data store
- Specify the number of Data Processing Units (DPUs) to allocate to your ETL job. 

**Durability and availability**
- Glue leverages the durability of the data stores to which you connect
- Provides job status and pushes notifications to CloudWatch events
- Use SNS notifications from CloudWatch events to notify of job failures or success

**Scalability and elasticity**
- Runs on top of the Apache Spark for transformation job scale-out execution

**Interfaces**
- Crawlers scan many data store types
- Bulk import Hive metastore into Glue Data Catalog

**Anti-patterns**
- Streaming data, unless Spark Streaming
- Heterogeneous ETL job types, i.e. if you want to use other engines (Hive, Pig, etc.) Data Pipeline, then EMR would be a better fit
- NoSQL databases: not supported as data source

**Security**
Glue supports
- Server-side Encryption (SSE) for data at rest
- SSL for data (in transit/motion)

**Monitoring**
- Fire off Lambda function or SNS for notification when ETL succeeds or fails
- Glue produces metrics for crawlers and jobs for monitoring
- Statistics are about the health of your environment and are written to the Glue Data Catalog
- Use automated monitoring tools to watch Glue and report problems
  - CloudWatch events
  - CloudWatch logs
  - CloudTrail logs


# AWS Glue Data Catalog

![]({{ site.url }}/assets/img/posts/glue_architecture.png)

- Glue Data Catalog is a central repository and persistent metadata store to store structural and operational metadata for all the data assets
- Glue Data Catalog provides a uniform repository where disparate systems can store and find metadata to keep track of data in data silos, and use that metadata to query and transform the data
- Table definitions once added to the Glue Data Catalog, are available for ETL and also readily available for querying in Athena, EMR, and Redshift Spectrum to provide a common view of the data between these services.
- Glue Data Catalog can serve as a Hive "metastore" and can also import a Hive metastore into Glue. Reminder: Hive lets you run SQL-like queries from EMR

# AWS Glue Crawler

- Glue Crawler connects to a data store, processes through a prioritized list of classifiers to extract the schema of the data and other statistics, and then populates the Glue Data Catalog with this metadata
- Glue Crawler scans various data stores to automatically infer schemas and partition structure to populate the Glue Data Catalog with corresponding table definitions and statistics.
- Glue Crawler can be scheduled to run periodically so that the metadata is always up-to-date and in-sync with the underlying data
- Glue Crawler automatically add new tables, new partitions to existing table, and new versions of table definitions
- Glue Crawler will extract paritions based on how your S3 data is organized.

# Glue ETL on Apache Spark

- Use Glue ETL when you do not need or want to pay for an EMR cluster
- Fully managed, cost effective, pay only for the resources consumed
- Jobs runs on a serverless Spark platform
- Glue ETL jobs use the concept of concurrent data processing units (DPUs) to describe the compute capacity of a job run. A single DPU provides four virtual central processing units (vCPUs) and 16 GB of memory.

## Glue ETL Jobs - Structure
A Glue job defines the business logic that performs the ETL work in AWS Glue

- Glue runs your script to extract data from your sources, transform it, clean it, enrich it and load it into your targets
- Glue triggers can start jobs based on a schedule or event, or on demand.
- Monitor your job runs to get runtime metrics: completion status, duration, etc.
- Glue ETL generates automatically code in Python or Scala, which you can modify
- Glue ETL provides your own Spark or PySpark scripts

## Glue ETL Jobs - Types
Glue output file formats JSON, CSV, ORC (Optimized Row Columnar), Apache Parquet, and Apache Avro
Three types of Glue jobs:
- Spark ETL job: executed in managed Apache Spark environment, processes data in batches
- Streaming ETL job: uses the Apache Spark Structured Streaming framework
- Python shell job: schedule and run tasks that do not require an Apache Spark environment

## Glue ETL Jobs - Transformations
Glue has built-in transforms for processing data
- Call from within your ETL script
- In a DynamicFrame (an extension of an Apache Spark SQL DataFrame), your data passes from transfrom to transform
- Built-in transform types (subset)
  - ApplyMapping: maps source DynamicFrame columns and data types to target DynamicFrame columns and data types
  - Filter: selects records from a DynamicFrame and returns a filtered DynamicFrame
  - Map: applies a function to the records of a DynamicFrame and returns a transformed DynamicFrame
  - Relationalize: converts a DynamicFrame to a relational (rows and columns) form
- Machine Learning Transformations
  - FindMatches ML: identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly
- Format conversions: CSV, JSON, Avro, Parquet, ORC, XML
- Apache Spark transformations, e.g. K-Means

## Glue ETL jobs - Triggers
A trigger can start specificed jobs and crawlers
- On demand, based on a schedule, or based on combination of events
- Add a trigger via the Glue console, the CLI or the Glue API
- Activate or deactivate a trigger via the Glue console, the CLI, or the Glue API

Profile your Glue jobs using metrics and visualize on the Glue and CloudWatch consoles to identify and fix issues

## Glue Job Bookmarks
- Persists state from the job run
- Prevents reprocessing of old data
- Allows you to proces new data only when re-running on schedule
- Works with S3 sources in a variety of formats
- Works with JDBC (if PK's are in sequential order)
  - IMPORTANT: Only handles new ros, NOT updated rows 

# AWS Glue Studio
- Visual interface for ETL workflows
- Visual job editor
  - Create DAG's for complex workflows
  - Data Sources include S3, Kinesis, Kafka, JDBC
  - Allows Transform, sample and join data
  - Target to S3 or Glue Data Catalog
  - Support partitioning
- Visual job dashboard

# AWS Glue DataBrew
- A visual data preparation tool to clean and normalize data
  - UI for pre-processing large data sets
  - Input from S3, data warehouse, or database
  - Output to S3
- Over 250 read-made transformations avaiable
- You create steps of transformations that can be saved as jobs within a larger project
- Security:
  - KMS
  - SSL in transit
  - IAM can restrict who can do what
  - CloudWatch & CloudTrail
