---
layout: post
title: Data Processing on AWS
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Data Processing, AWS]
mathjax: true
summary: Data Processing on AWS
---


# Glue ETL on Apache Spark

- Use Glue when you do not need or want to pay for an EMR cluster
- Glue generates an Apache Spark (PySpark or Scala) script
- Glue runs in a fully managed Apache Spark environment

- Spark has 4 primary libraries:
  - Spark SQL
  - Spark Streaming
  - MLlib
  - GraphX

## Glue ETL Jobs - Structure
A Glue job defines the business logic that performs the ETL work in AWS Glue
- Glue runs your script to extract data from your sources, transform the data, and load it into your targets
- Glue triggers can start jobs based on a schedule or event, or on demand.
- Monitor your job runs to get runtime metrics: completion status, duration, etc.
- Based on your source schema and target location or schema, the Glue code generator automatically creates an Apache Spark API (PySpark) script.
  - Edit the script to customize to your requirements.

## Glue ETL Jobs - Types
Glue output file formats JSON, CSV, ORC (Optimized Row Columnar), Apache Parquet, and Apache Avro
Three types of Glue jobs:
- Spark ETL job: executed in managed Apache Spark environment, processes data in batches
- Streaming ETL job: uses the Apache Spark Structured Streaming framework
- Python shell job: schedule and run tasks that do not require an Apache Spark environment

## Glue ETL Jobs - Transforms
Glue has built-in transforms for processing data
- Call from within your ETL script
- In a DynamicFrame (an extension of an Apache Spark SQL DataFrame), your data passes from transfrom to transform
- Built-in transform types (subset)
  - ApplyMapping: maps source DynamicFrame columns and data types to target DynamicFrame columns and data types
  - Filter: selects records from a DynamicFrame and returns a filtered DynamicFrame
  - Map: applies a function to the records of a DynamicFrame and returns a transformed DynamicFrame
  - Relationalize: converts a DynamicFrame to a relational (rows and columns) form

## Glue ETL jobs - Triggers
A trigger can start specificed jobs and crawlers
- On demand, based on a schedule, or based on combination of events
- Add a trigger via the Glue console, the CLI or the Glue API
- Activate or deactivate a trigger via the Glue console, the CLI, or the Glue API

## Glue ETL jobs - Monitoring
Glue produces metrics for crawlers and jobs for monitoring
- Statistics about the health of your environment
- Statistics are written to the Glue Data Catalog

Use automated monitoring tools to watch Glue and report problems
- CloudWatch events
- CloudWatch logs
- CloudTrail logs

Profile your Glue jobs using metrics and visualize on the Glue and CloudWatch consoles to identify and fix issues


# EMR Cluster ETL Processing

- More flexible and powerful than Spark
- Can use Spark on EMR, but also have other options:
  - Flink: Framework and distributed processing engine for stateful computations over unbounded and bounded data streams
  - Hadoop: Framework that allows for the distributed processing of large data sets across clusters of computers
  - Spark: Distributed general-purpose cluster-computing framework
  - Sqoop: Command-line interface application for transferring data between relational databases and Hadoop
  - TensorFlow: Machine Learning library
  - Zepplin: Web-based notebook for data-driven analytics
  - Hive: A SQL-like interface to query data stored in various databases and file systems that integrates with Hadoop. Alternative to Glue Data Catalog
  - Hue: Web interface for analyzing data with Hadoop
  - Presto: High performance distributed SQL query engine for big data

## EMR Components

EMR is built upon clusters (or collections) of EC2 instances
- The EC2 instances are called nodes, all of which have roles (or node type) in the cluster
- EMR installs different software components on each node type, defining the node's role in the distributed architecture of EMR
- Three types of nodes in an EMR cluster
  - Master node: manages the cluster, running software components to coordinate the distribution of data and tasks across other nodes for processing
  - Core node: has software components that run tasks and store data in the HDFS on your cluster
  - Task node: node with software components that ONLY runs tasks and does NOT store data in HDFS

## EMR Cluster - Work
Options for submitting work to your EMR cluster
- Script the work to be done as functions that you specify in the steps you define when you create a cluster
  - This approach is used for clusters that process data and then terminate
- Build a long-running cluster and submit steps (containing one or more jobs( to it via the console, the EMR API, or the AWS CLI
  - This approach is used for clusters that process data continuously or need to remain available
- Create a cluster and connect to the master node and/or other nodes as required using SSH
  - This approch is used to perform tasks and submit queries, either scripted or interactively, via the interfaces of the installed applications

## EMR Cluster - Processing Data
At launch time, you choose the framework and applications to install to achieve your data prcoessing needs.
You submit jobs or queries to installed applications or run steps to process data in your EMR cluster
- Submitting jobs/steps to installed applications
  - Each step is a unit of work that has instructions to process data by software installed on the cluster
  - Example: Submit a request to run a set of steps

## EMR Cluster - Lifecycle

1. Provisions EC2 instances of the cluster
2. Runs bootstrap actions
3. Installs applications such as Hive, Hadoop, Sqoop, Spark
4. Connect to the cluster instances; cluster sequentially runs any steps that specified at creation; submit additional steps
5. After steps complete the cluster waits or shuts down, depending on config
6. When all instances are terminated, the cluster moves to COMPLETED state

## EMR Architecture - Storage
Architected in layers.
Storage: file systems used by the cluster
- HDFS: distributes the data it stores across instances in the cluster (ephemeral)
- EMR File System (EMRFS): directly access data stored in S3 as if it were a file system like HDFS
- Local file system: EC2 locally connected disk

## EMR Architecture - Cluster Management
YARN (Yet Another Resource Manager)
- Centrally manage cluster resources
- Agent on each node that keeps the cluster healthy, and communicates with EMR
- EMR defaults to scheduling YARN jobs so that jobs will not fail when task nodes running on spot instances are terminated

## EMR Architecture - Data Processing Frameworks
Framework layer that is used to process and anaylze data
Different frameworks are available:
- Hadoop MapReduce:
  - Parallel distributed applications that use Map and Reduce functions (e.g. Hive)
    - Map function maps data to sets of key-value pairs
    - Reduce function combines the key-value pairs and processes the data
  - Spark
    - Cluster framework and programming model for processing big data workloads

## EMR Architecture - Application Programs
- Supports many application programs
- Create processing workloads
- Use machine learning algorithms
- Create stream processing apps
- Build data warehouses and data lakes

## EMR Applications and Categories
EMR supports many hadoop applications
- Spark
- Hive
- Presto
- HBase

Data scientists and engineers use EMR to run analytics workflows using these tools along with Hue and EMR Notebooks

Several important categories of EMR applications
- Data Processing 
- SQL
- NoSQL
- Interactive Analytics

### Data Processing Applications

Apache Spark
- Hadoop ecosystem engine used process large data sets very quickly
- Runs fault-tolerant resilient distributed datasets (RDDs) in-memory, and defines data transformations
- Includes Spark SQL, Spark Streaming, MLlib, and GraphX

Apache Flink
- Streaming dataflow engine that allows you to run real-time stream processing on high-throughput data sources
- Supports APIs optimized for writing both streaming and batch applications

### SQL Applications

Apache Hive
- Data warehouse and analytics package that runs on top of Hadoop
- Allows you to structure, summarize, and query data
- Supports map/reduce functions and complex extensible user-defined data types like JSON and Thrift
- Can process complex and unstructured data sources such as text documents and log files

Presto
- SQL query engine optimized for low-latency, ad-hoc analysis of data
- Can process data from multiple data sources including the HDFS and Amazon S3

### NoSQL Applications
Apache HBase
- Non-relational, distributed database modeled after Google's BigTable
- Runs on top of HDFS to provide BigTable-like capabilities for Hadoop
- Store large amounts of sparse data using column-based compression and storage
- Use S3 as a data store for HBase, enabling you to lower costs and reduce operational complexity

### Interactive Analytics Applications
Hue
- User Interface for Hadoop that allows you to create and run Hive queries, manage files in HDFS, create and run Pig scripts, and manage tables
- Also integrates with S3, so you can query directly against S3 and easily transfer files between HDFS and Amazon S3

EMR Notebooks
- Notebooks pre-configured for Spark that are based on Jupyter notebooks
- Interactively run Spark jobs on EMR clusters written in PySpark, Spark SQL, Spark R or Scala


## Integrates with the following data stores
- Use S3 as an object store for Hadoop
- HDFS on the Core nodes instance storage
- Directly access and process data in DynamoDB
- Process data in RDS
- Use COPY command to load data in parallel into Redshift from EMR
- Integrates with S3 Glacier for low-cost storage


# Kinesis ETL (Streaming) Processing
- Use Kinesis Analytics to gain real-time insights into your streaming data
  - Query data in your stream or build streaming applications using SQL or Flink
  - Use for filtering, aggregation, and anomaly detection
  - Preprocess your data with lambda
