---
layout: post
title: Data Lake
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Data Lake, AWS]
mathjax: true
summary: How to create and operate a data lake in a secure and scalable way
---


# Data Lakes

In this post we will understand what exactly a data lake is how to use AWS data related services to create and 
operate a data lake in a secure and scalable way.

What is a Data Lake?
A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. 
You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards 
and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.

### Value Propposition of Data Lakes
Why to use data lakes? 

It may be the case that you have an enormous database or (structured, semi-structured and unstructured) data is distributed across multple databases. 
The first case, you may have typical load issues, in the second case, data cataloging and security may be challenging.

The main reasons to invest in a data lake are:
- to increase operational efficency
- make data available from departmental silos
- lower transactional costs, and
- offload capacity from databases and data warehouses.

One of the most significant advantages of a data lake is storing our data without having to think about its structure. 
That's where the name lake comes from, being organic information, just like nature.  By decoupling storage from data processing, 
you can use different data processing and data visualization components that look at the same data, and that's another advantage 
of data lakes over databases or data warehouses.


### Characteristics of Data Lakes
Data lake encompasses multiple functions in the data spectrum, such as data ingestion, processing, machine learning, data visualization, and other components. 

1. Data Lakes should be data-agnostic
The first aspect is that data lake should be data-agnostic. In other words, data lakes should not be limited to store 
 just one file type or data structure like usually a database do. Efficient data lakes allow you to store all kinds of data, such as pictures, data text 
 in multiple formats, compressed or not, encrypted or not, audio, video, binary files, and everything else.

Being data-agnostic and storing raw data gives you the advantage of using a more appropriate data processing tool to generate insights.

2. Data Lakes are future proof
 In a nutshell, data lakes allow you to dive in anywhere in the vast data collected to generate insights. 
 We usually say that another characteristic of mature data lakes is the fact of being future proof.
Let me better explain this. Being future proof here usually indicates that you may not have a business question today, 
but if that business question arises in the future and you have data around that topic, the answer can potentially be right there. 
With the re-use of the right processing and data visualization tools, you can generate insights that can help you a lot. 

With correct data ingestion, cataloging, and processing to keep it organized, it will be delightful to extract insights from it.

### Components of Data Lakes
The four main components we would like to have in our data lake architecture:
- Ingest and store
- catalog and search
- process and serve
- protect and secure

#### Ingest and store
A data lake is not a mature data lake without proper data ingestion mechanisms, because data lakes are datagnostic.

Use Case

Imagine a company with a fleet of 100 trucks equipped with IoT devices. 
Let's say the devices are equipped with GPS antennas and mobile connections providing the truck coordinates every second. 
When this data is submitted, the system should provide trucks location in real time to it's users. If you do the math, 
with 100 trucks providing data every second, you have 6000 datapoints per minutes, 360,000 datapoints every hour, 
and way more than a million datapoints just during business hours. This requirement may need data ingestion and storage solutions 
operating at a minimal latency on a large scale, which clearly is a challenge, especially if you don't use the right approach. 
In this race for the lower latency, a better ingestion layer likely uses data streaming technologies such as services from the 
Amazon Kinesis family or Amazon Managed Streaming for Apache Kafka. 

Perhaps we could use a different data ingestion mechanism if the requirements were different in this scenario. 
If we did not need to provide near real time insights to users, we could think about using an ingestion mechanism that consumes 
data in batches and not streaming.

#### Catalog and search
Mature data lakes provide efficient indexing and searching mechanisms to quickly discover what data is stored and where.
Data lakes can and should reach a high level of organization and catalog with the help of databases. 
Regardless of whether the databases is SQL or no SQL, it is very important to have a quick and secure way to find 
your data when you're looking for something.

#### Process and serve
 One of the main components of cloud computing is paying for what you use and nothing more. So if you just want to use Hadoop for storage, 
 you may be paying for processing power while not using it. That does not mean Hadoop is not using data lakes, they play a big role and data lakes 
 would not be a thing without it, but it makes more sense to use Hadoop only when we need to process something. That's why a cloud-native data lake uses 
 dedicated storage services that only charge you for storage and nothing else. 
 Typically in data lakes when you need to process something, you can spin up what we call a transient Hadoop cluster, do the processing, 
 store the data back on S3 and then turn off the cluster. In addition to that, separating compute from storage gives you another benefit, 
 which is the ability to use different compute and storage layers using the right tool for the job while optimizing for cost as well as performance. 
 AWS has a service called AWS Glue which execute Hadoop jobs on demand where you only pay when there is a job running.

#### Protect and secure
 A mature data lake should provide all the components mentioned so far in a secure way with configurable permission mechanisms that prevent unauthorized users 
 from accessing the data. Using AWS managed services gives you the ability to manage things like data retention, encryption, access control lists and refined 
 governance standards.

### What is the difference between Data Lakes, Data Warehousing and Databases.

**Database**

Databases usually have its storage and processing mechanisms tied together, making it less flexible to-scale storage without processing and vice versa.

**Data Warehouse**

1. Schema-on-read vs schema-on-write
A data warehouse is usually a database optimized to perform analytical queries that leads to insights. But because it usually operates as an analytical database, 
you need to create tables and define the table structure before adding your data into your data warehouse. 
 When you create those tables, you have to set the table columns and data types, in order words, a data schema that generally needs information to be structured. 
 When the schema needs to be populated and it needs to be determined before you write the data, you have what we call a schema-on-write architecture. 
 Although schema-on-write is good for data normalization because it would reject data that does not fit in that specific format, it is not ideal for flexibility, 
 which is where data lakes really shine. Data lakes are what we call schema-on-read. And that's the first fundamental difference between data lakes and data warehouses. 
 Data lakes can handle unstructured data and mainly operate in a schema-on-read fashion. Which means that you do not need to concern with the data schema while ingesting 
 the data to your data lake. That allow you to take care of the schema only when read data for some future processing. Hence the name schema-on-read. 
 AWS has a service called Amazon Athena that allow you to match the table schema to your dataset and not the datasets to your table schema. 
 And Amazon Athena table does not store data on itself. The tables are mostly metadata pointing to where the data is, and we commonly use Amazon S3 for that. With that being said, 
 if something changes in terms of data structure in S3, you can change it in Athena table to match that change. Typically we end up creating another table in Amazon Athena.

I'm defining that a data lake uses schema-on-read and data warehouses uses schema-on-write. Having schema-on-read on your data lakes gives you the flexibility that you need.

2. Data Warehouses mostly use SQL
data warehouses mostly use the SQL as the language for querying. That limits what you can do, and some engines even support the creation of user-defined functions and other functionalities 
to extend that a little bit. Although that is good, having a custom compute framework based in a programming language like Spark and Python gives you more power and flexibility. 
Because if you have statistical data, you can use compute frameworks that are exclusively focused on statistics. The same applies with machine learning, computer vision, 
and other needs that you may have for your data lake, allowing you to reuse the right tool for the right job.

3. Data warehouses work with structured data only, data lakes work with unstructured and structured data natively
Since data needs to feed in a table in a data warehouse, you have to normalize and preformat that data to fit that table. And that limits you on the type of data that you can store in a table in your data warehouse. 

Now, one thing is for sure, although data lakes and data warehouses are not the same thing, they should be used together to extract valuable insights. It is very common to see architectures ingesting 
data to the data lake first, then cleaning and storing that data using data lake tools, and then storing that data into data warehouses that uses data visualization tools. 
Some data warehouses may work faster because they work with structured data.


![]({{ site.url }}/assets/img/posts/DWH_vs_DL.png)

![]({{ site.url }}/assets/img/posts/DWH_vs_DB.png)

### Data Lake Architectures



### AWS Data Related Services for Data Lake Storage and Cataloging, Data Movement, Data Analytics and Processing
Let's start exploring differen AWS services, used in data lake solutions.

AWS has a wide variety of services that you can use when building out your data lake.

AWS build services in a modular way, it means architecture diagrams for data lakes can have a lot going on and involve a good amount of AWS services working together. Think of the services that can pose your data lake as belonging to one of three categories. 

1. Data storage and cataloguing
A data lake stores and secures data at an unlimited scale. It allows for structured and unstructured data to be stored together. It catalogs and indexes data without data movement, and it connects data with services and tooling for analysis and processing.
This category focuses on the services used to build the data lake itself.

2. Data Movement
Once you know what services you are going to use to build out your data lake how are you going to ingest data into that data lake? Well, you have to move the data from its origin or source into the data lakes so it can then be indexed and made available for processing and analysis.

3. Analytics and Processing
The question then becomes, when and what services can you use to process and analyze data. You might what to run sequel queries on the data, maybe join multiple datasets together to feed into a dashboard for visualization or even use the data to generate things like business reports. 

There are benefits to thinking of your data lake as having decoupled layers like this. It means you can manage and scale and secure each individual building block for your data lake independently. Keeping compute and storage separate from each other is a very powerful concept that you should keep in mind.



### Amazon S3 is the storage layer of choice for data lakes hosted on AWS

We could have thousands of terabytes of data that we need to store, and we know that we will need to use services that will scale up as our data set grows, as well as allows us to store both unstructured and structured data together. What options do we have? Initially, we might consider storing our data in a relational database, since databases can be both indexed and queried. But as we already learned in an earlier lesson, this is not feasible. 

We need a service that allowed this schema on read model to work while also having builtin- scaling features that make hosting a data Lake easy.

### Amazon S3
Amazon Simple Storage Service is storage for the Internet. It is designed to make web-scale computing easier for developers.

Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. The service aims to maximize benefits of scale and to pass those benefits on to developers. 

Amazon S3 is the largest and most performant object storage service for structured and unstructured data and the storage service of choice to build a data lake. With Amazon S3, you can cost-effectively build and scale a data lake of any size in a secure environment where data is protected by 99.999999999% (11 9s) of durability.   
On top of S3 being scalable, durable and offering integrations for analysis, it also has some specific features we can use to help bring down the cost of operating our data Lake. S3 offers different storage tiers that allow us to categorise our data into different cost tiers based on how frequently it will be accessed, the way it

Read more about crawlers and classifiers [here:](https://aws.amazon.com/products/storage/data-lake-storage/)

### Amazon S3 Storage Classes
For performance-sensitive use cases (those that require millisecond access time) and frequently accessed data, Amazon S3 provides the following storage classes: S3 Standard—The default storage class. If you don't specify the storage class when you upload an object, Amazon S3 assigns the S3 Standard storage class. Reduced Redundancy—The Reduced Redundancy Storage (RRS) storage class is designed for noncritical, reproducible data that can be stored with less redundancy than the S3 Standard storage class. 

The S3 Standard-IA and S3 One Zone-IA storage classes are designed for long-lived and infrequently accessed data. (IA stands for infrequent access.) S3 Standard-IA and S3 One Zone-IA objects are available for millisecond access (same as the S3 Standard storage class). Amazon S3 charges a retrieval fee for these objects, so they are most suitable for infrequently accessed data.   

The S3 Glacier and S3 Glacier Deep Archive storage classes are designed for low-cost data archiving. These storage classes offer the same durability and resiliency as the S3 Standard storage class.   

The S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving data to the most cost-effective storage access tier, without performance impact or operational overhead. S3 Intelligent-Tiering delivers automatic cost savings by moving data on a granular object level between two access tiers, a frequent access tier and a lower-cost infrequent access tier, when access patterns change. The Intelligent-Tiering storage class is ideal if you want to optimize storage costs automatically for long-lived data when access patterns are unknown or unpredictable.   

Read more about crawlers and classifiers [here:](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)

### AWS Glue Data Catalog
One of the main issues organizations run into when trying to create and operate a data lake is they might find that they have a large amount of raw assets. But the users of the data lake aren't really sure what that data is, or even how to find out what data exists for them to begin analyzing and processing. On top of that, data will be transformed overtime. Keeping track of the changes to the data formats and the versions of the data can be confusing for users when it isn't properly managed. They aren't sure if the data they need is in the data lake or not. This means that the data, though it exists, is not very useful. When the data is not easily turned into insight, you might have a data swamp. How do you fix this data swamp problem?

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so there’s no infrastructure to set up or manage.   

The AWS Glue Data Catalog is your persistent metadata store. It is a managed service that lets you store, annotate, and share metadata in the AWS Cloud in the same way you would in an Apache Hive metastore.    

AWS Glue also lets you set up crawlers that can scan data in all kinds of repositories, classify it, extract schema information from it, and store the metadata automatically in the AWS Glue Data Catalog. The AWS Glue Data Catalog can then be used to guide ETL operations.   

Crawlers use classifiers, a classifier reads the data in a data store. If it recognizes the format of the data, it generates a schema. The classifier also returns a certainty number to indicate how certain the format recognition was.  

Read more about crawlers and classifiers [here:](https://docs.aws.amazon.com/glue/latest/dg/add-classifier.html)
