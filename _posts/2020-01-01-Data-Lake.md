---
layout: post
title: Data Lake
featured-img: shane-rounce-205187
image: shane-rounce-201587
categories: [Data Lake, AWS]
mathjax: true
summary: How to create and operate a data lake in a secure and scalable way
---


# Data Lakes (WORK IN PROGRESS POST)

In this post we will understand what exactly a data lake is how to use AWS data related services to create and 
operate a data lake in a secure and scalable way.

What is a Data Lake?
A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. 
You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards 
and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.

### Value Propposition of Data Lakes
Why to use data lakes? 

It may be the case that you have an enormous database or (structured, semi-structured and unstructured) data is distributed across multple databases. 
The first case, you may have typical load issues, in the second case, data cataloging and security may be challenging.

The main reasons to invest in a data lake are:
- to increase operational efficency
- make data available from departmental silos
- lower transactional costs, and
- offload capacity from databases and data warehouses.

One of the most significant advantages of a data lake is storing our data without having to think about its structure. 
That's where the name lake comes from, being organic information, just like nature.  By decoupling storage from data processing, 
you can use different data processing and data visualization components that look at the same data, and that's another advantage 
of data lakes over databases or data warehouses.


### Characteristics of Data Lakes
Data lake encompasses multiple functions in the data spectrum, such as data ingestion, processing, machine learning, data visualization, and other components. 

1. Data Lakes should be data-agnostic
The first aspect is that data lake should be data-agnostic. In other words, data lakes should not be limited to store 
 just one file type or data structure like usually a database do. Efficient data lakes allow you to store all kinds of data, such as pictures, data text 
 in multiple formats, compressed or not, encrypted or not, audio, video, binary files, and everything else.

Being data-agnostic and storing raw data gives you the advantage of using a more appropriate data processing tool to generate insights.

2. Data Lakes are future proof
 In a nutshell, data lakes allow you to dive in anywhere in the vast data collected to generate insights. 
 We usually say that another characteristic of mature data lakes is the fact of being future proof.
Let me better explain this. Being future proof here usually indicates that you may not have a business question today, 
but if that business question arises in the future and you have data around that topic, the answer can potentially be right there. 
With the re-use of the right processing and data visualization tools, you can generate insights that can help you a lot. 

With correct data ingestion, cataloging, and processing to keep it organized, it will be delightful to extract insights from it.

### Components of Data Lakes
The four main components we would like to have in our data lake architecture:
- Ingest and store
- catalog and search
- process and serve
- protect and secure

#### Ingest and store
A data lake is not a mature data lake without proper data ingestion mechanisms, because data lakes are datagnostic.

Use Case

Imagine a company with a fleet of 100 trucks equipped with IoT devices. 
Let's say the devices are equipped with GPS antennas and mobile connections providing the truck coordinates every second. 
When this data is submitted, the system should provide trucks location in real time to it's users. If you do the math, 
with 100 trucks providing data every second, you have 6000 datapoints per minutes, 360,000 datapoints every hour, 
and way more than a million datapoints just during business hours. This requirement may need data ingestion and storage solutions 
operating at a minimal latency on a large scale, which clearly is a challenge, especially if you don't use the right approach. 
In this race for the lower latency, a better ingestion layer likely uses data streaming technologies such as services from the 
Amazon Kinesis family or Amazon Managed Streaming for Apache Kafka. 

Perhaps we could use a different data ingestion mechanism if the requirements were different in this scenario. 
If we did not need to provide near real time insights to users, we could think about using an ingestion mechanism that consumes 
data in batches and not streaming.

#### Catalog and search
Mature data lakes provide efficient indexing and searching mechanisms to quickly discover what data is stored and where.
Data lakes can and should reach a high level of organization and catalog with the help of databases. 
Regardless of whether the databases is SQL or no SQL, it is very important to have a quick and secure way to find 
your data when you're looking for something.

#### Process and serve
 One of the main components of cloud computing is paying for what you use and nothing more. So if you just want to use Hadoop for storage, 
 you may be paying for processing power while not using it. That does not mean Hadoop is not using data lakes, they play a big role and data lakes 
 would not be a thing without it, but it makes more sense to use Hadoop only when we need to process something. That's why a cloud-native data lake uses 
 dedicated storage services that only charge you for storage and nothing else. 
 Typically in data lakes when you need to process something, you can spin up what we call a transient Hadoop cluster, do the processing, 
 store the data back on S3 and then turn off the cluster. In addition to that, separating compute from storage gives you another benefit, 
 which is the ability to use different compute and storage layers using the right tool for the job while optimizing for cost as well as performance. 
 AWS has a service called AWS Glue which execute Hadoop jobs on demand where you only pay when there is a job running.

#### Protect and secure
 A mature data lake should provide all the components mentioned so far in a secure way with configurable permission mechanisms that prevent unauthorized users 
 from accessing the data. Using AWS managed services gives you the ability to manage things like data retention, encryption, access control lists and refined 
 governance standards.

### What is the difference between Data Lakes, Data Warehousing and Databases.

**Database**

Databases usually have its storage and processing mechanisms tied together, making it less flexible to-scale storage without processing and vice versa.

**Data Warehouse**

1. Schema-on-read vs schema-on-write
A data warehouse is usually a database optimized to perform analytical queries that leads to insights. But because it usually operates as an analytical database, 
you need to create tables and define the table structure before adding your data into your data warehouse. 
 When you create those tables, you have to set the table columns and data types, in order words, a data schema that generally needs information to be structured. 
 When the schema needs to be populated and it needs to be determined before you write the data, you have what we call a schema-on-write architecture. 
 Although schema-on-write is good for data normalization because it would reject data that does not fit in that specific format, it is not ideal for flexibility, 
 which is where data lakes really shine. Data lakes are what we call schema-on-read. And that's the first fundamental difference between data lakes and data warehouses. 
 Data lakes can handle unstructured data and mainly operate in a schema-on-read fashion. Which means that you do not need to concern with the data schema while ingesting 
 the data to your data lake. That allow you to take care of the schema only when read data for some future processing. Hence the name schema-on-read. 
 AWS has a service called Amazon Athena that allow you to match the table schema to your dataset and not the datasets to your table schema. 
 And Amazon Athena table does not store data on itself. The tables are mostly metadata pointing to where the data is, and we commonly use Amazon S3 for that. With that being said, 
 if something changes in terms of data structure in S3, you can change it in Athena table to match that change. Typically we end up creating another table in Amazon Athena.

I'm defining that a data lake uses schema-on-read and data warehouses uses schema-on-write. Having schema-on-read on your data lakes gives you the flexibility that you need.

2. Data Warehouses mostly use SQL
data warehouses mostly use the SQL as the language for querying. That limits what you can do, and some engines even support the creation of user-defined functions and other functionalities 
to extend that a little bit. Although that is good, having a custom compute framework based in a programming language like Spark and Python gives you more power and flexibility. 
Because if you have statistical data, you can use compute frameworks that are exclusively focused on statistics. The same applies with machine learning, computer vision, 
and other needs that you may have for your data lake, allowing you to reuse the right tool for the right job.

3. Data warehouses work with structured data only, data lakes work with unstructured and structured data natively
Since data needs to feed in a table in a data warehouse, you have to normalize and preformat that data to fit that table. And that limits you on the type of data that you can store in a table in your data warehouse. 

Now, one thing is for sure, although data lakes and data warehouses are not the same thing, they should be used together to extract valuable insights. It is very common to see architectures ingesting 
data to the data lake first, then cleaning and storing that data using data lake tools, and then storing that data into data warehouses that uses data visualization tools. 
Some data warehouses may work faster because they work with structured data.


![]({{ site.url }}/assets/img/posts/DWH_vs_DL.png)

![]({{ site.url }}/assets/img/posts/DWH_vs_DB.png)

### Data Lake Architectures



### AWS Data Related Services for Data Lake Storage and Cataloging, Data Movement, Data Analytics and Processing
Let's start exploring differen AWS services, used in data lake solutions.

AWS has a wide variety of services that you can use when building out your data lake.

AWS build services in a modular way, it means architecture diagrams for data lakes can have a lot going on and involve a good amount of AWS services working together. Think of the services that can pose your data lake as belonging to one of three categories. 

1. Data storage and cataloguing
A data lake stores and secures data at an unlimited scale. It allows for structured and unstructured data to be stored together. It catalogs and indexes data without data movement, and it connects data with services and tooling for analysis and processing.
This category focuses on the services used to build the data lake itself.

2. Data Movement
Once you know what services you are going to use to build out your data lake how are you going to ingest data into that data lake? Well, you have to move the data from its origin or source into the data lakes so it can then be indexed and made available for processing and analysis.

3. Analytics and Processing
The question then becomes, when and what services can you use to process and analyze data. You might what to run sequel queries on the data, maybe join multiple datasets together to feed into a dashboard for visualization or even use the data to generate things like business reports. 

There are benefits to thinking of your data lake as having decoupled layers like this. It means you can manage and scale and secure each individual building block for your data lake independently. Keeping compute and storage separate from each other is a very powerful concept that you should keep in mind.



### Amazon S3 is the storage layer of choice for data lakes hosted on AWS

We could have thousands of terabytes of data that we need to store, and we know that we will need to use services that will scale up as our data set grows, as well as allows us to store both unstructured and structured data together. What options do we have? Initially, we might consider storing our data in a relational database, since databases can be both indexed and queried. But as we already learned in an earlier lesson, this is not feasible. 

We need a service that allowed this schema on read model to work while also having builtin- scaling features that make hosting a data Lake easy.

### Amazon S3
Amazon Simple Storage Service is storage for the Internet. It is designed to make web-scale computing easier for developers.

Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. The service aims to maximize benefits of scale and to pass those benefits on to developers. 

Amazon S3 is the largest and most performant object storage service for structured and unstructured data and the storage service of choice to build a data lake. With Amazon S3, you can cost-effectively build and scale a data lake of any size in a secure environment where data is protected by 99.999999999% (11 9s) of durability.   
On top of S3 being scalable, durable and offering integrations for analysis, it also has some specific features we can use to help bring down the cost of operating our data Lake. S3 offers different storage tiers that allow us to categorise our data into different cost tiers based on how frequently it will be accessed, the way it

Read more about crawlers and classifiers [here:](https://aws.amazon.com/products/storage/data-lake-storage/)

### Amazon S3 Storage Classes
For performance-sensitive use cases (those that require millisecond access time) and frequently accessed data, Amazon S3 provides the following storage classes: S3 Standard—The default storage class. If you don't specify the storage class when you upload an object, Amazon S3 assigns the S3 Standard storage class. Reduced Redundancy—The Reduced Redundancy Storage (RRS) storage class is designed for noncritical, reproducible data that can be stored with less redundancy than the S3 Standard storage class. 

The S3 Standard-IA and S3 One Zone-IA storage classes are designed for long-lived and infrequently accessed data. (IA stands for infrequent access.) S3 Standard-IA and S3 One Zone-IA objects are available for millisecond access (same as the S3 Standard storage class). Amazon S3 charges a retrieval fee for these objects, so they are most suitable for infrequently accessed data.   

The S3 Glacier and S3 Glacier Deep Archive storage classes are designed for low-cost data archiving. These storage classes offer the same durability and resiliency as the S3 Standard storage class.   

The S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving data to the most cost-effective storage access tier, without performance impact or operational overhead. S3 Intelligent-Tiering delivers automatic cost savings by moving data on a granular object level between two access tiers, a frequent access tier and a lower-cost infrequent access tier, when access patterns change. The Intelligent-Tiering storage class is ideal if you want to optimize storage costs automatically for long-lived data when access patterns are unknown or unpredictable.   

Read more about crawlers and classifiers [here:](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)

### AWS Glue Data Catalog
One of the main issues organizations run into when trying to create and operate a data lake is they might find that they have a large amount of raw assets. But the users of the data lake aren't really sure what that data is, or even how to find out what data exists for them to begin analyzing and processing. On top of that, data will be transformed overtime. Keeping track of the changes to the data formats and the versions of the data can be confusing for users when it isn't properly managed. They aren't sure if the data they need is in the data lake or not. This means that the data, though it exists, is not very useful. When the data is not easily turned into insight, you might have a data swamp. How do you fix this data swamp problem?

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so there’s no infrastructure to set up or manage.   

The AWS Glue Data Catalog is your persistent metadata store. It is a managed service that lets you store, annotate, and share metadata in the AWS Cloud in the same way you would in an Apache Hive metastore.    

AWS Glue also lets you set up crawlers that can scan data in all kinds of repositories, classify it, extract schema information from it, and store the metadata automatically in the AWS Glue Data Catalog. The AWS Glue Data Catalog can then be used to guide ETL operations.   

Crawlers use classifiers, a classifier reads the data in a data store. If it recognizes the format of the data, it generates a schema. The classifier also returns a certainty number to indicate how certain the format recognition was.  

Read more about crawlers and classifiers [here:](https://docs.aws.amazon.com/glue/latest/dg/add-classifier.html)

### AWS Services Used for Data Movement
The data in a data lake will likely becoming from multiple sources. The way you move or ingest that data will depend on the source and the type of the data that you are ingesting. This process allows you to scale to data of any size, while saving time of defining data structures, schema, and transformations.   

Sharing data in the Cloud lets users put focus on analysis and deriving insights. To wrap this up. Just like real lakes that are formed in various ways or are formed in a combination of ways. Your data lake on AWS will be formed by data coming from various places and the tools that you use for data ingestion will vary as well. You can use a service from the Kinesis family for real-time data ingestion, Amazon API Gateway for RESTful data ingestion, App flow for data ingestion from SaaS applications, Data Exchange, or the Registry of open data for public datasets that you want to ingest into your data lake.

### Amazon Kinesis 
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.     

There are multiple services in the Amazon Kinesis family. For data ingestion, there is Amazon Kinesis Data Streams, Amazon Kinesis Video Streams, and Amazon Kinesis Data Firehose. 

Read more about Amazon Kinesis [here:](https://aws.amazon.com/kinesis/)   


### Amazon API Gateway
Amazon API Gateway is a fully managed service that makes it easy to create, publish, and maintain secure APIs at scale. APIs are the front door to backend applications and services. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management.

Read more about API Gateway [here:](https://aws.amazon.com/api-gateway/)


### Amazon Kinesis

Amazon Kinesis is a scalable and durable real-time data streaming service to ingest and analyze data in real-time from multiple data sources. Its Amazon’s fully managed service for collecting, processing and analyzing streaming data in the cloud. Some examples where Kinesis can be used streaming data are trading data, geospatial data, social networks data, any realtime monitoring solutions etc.

There are 4 different types of Kinesis streams
1. Kinesis Data Streams
2. Kinesis Firehose Delivery Streams
3. Kinesis Video Analytics
4. Kinesis Data Analytics

#### Data Streaming Ingestion with Kineses Services
Producers will produce data from different sources and ingested into the Kinesis Data Stream and it distributes the data amongst its shards and finally send it to the consumers. Consumers need to be configured by us programmatically as per the requirement. We can have multiple consumers. When data enters the data stream, it can be persistent from 24 hours to 168 hours before it disappears from the stream. The way you pay is as per the running shard.


**Top three observations**
1. Amazon Kinesis is data agnostic
Different data producers sending data to the same Kinesis data stream. You can send JSON, or XML, structured or unstructured data. We may want to create one Kinesis Data Stream per data type, or concentrate data in the same Kinesis Data Streams like We are doing in this diagram. The way you design your architectures is up to us.

2. Data retention period and data consumption
The data remains in the Kinesis Data Streams. Regardless if it has been consumed or not. Data stays there until it expires, according to what we call a data retention period. You can extend that by doing service API calls, which would also increase the hourly cost off the Kinesis data stream. Keeping the data in Kinesis is valuable because multiple consumers can consume the same data at the same time. In addition to that, it allows replaying the data consumption in case off data consumer failures.  If that application crashes, you can run in again and set a past start position. If that past start position is within the data retention window configured in the Kenisis stream, your application can consume data added to the stream while it was offline. 

3. Pull-based mechanism
Kinesis does not work as a push-based delivery mechanism. If a consumer wants to consume Kinesis data, it must initiate a connection using the Kinesis Client Library available for most popular programming languages. That code can run from on-premises, from within an EC2 instance, from your laptop, or from within a lambda function. 

People usually write Kinesis consumers for two main reasons.
1. Getting the data and placing it somewhere, such as a storage service. 
2. Performing some real time analysis off data that it's passing on the stream.

What is a shard?
A stream can be composed of one or more shards. One shard can read data at a rate of up to 2 MB/sec and can write up to 1,000 records/sec up to a max of 1 MB/sec. A user should specify the number of shards that coincides with the amount of data expected to be present in their system. Pricing of Kinesis streams is done on a per/shard basis



#### Amazon Kinesis Firehose

AWS realized that most customers were writing Kinesis consumers just to get data and move to S3 with minimal modification, it's just compression or encryption. To make your life easier, we created Amazon Kinesis Firehose.

It is similar to data stream, rather it is lot simpler. The difference is as soon as the data consumed by consumers, it immediately disappears from the queue, so no persistent of data.
Firehose this part off Amazon Kinesis family, and helps you have data available for multiple destinations. With a few mouse clicks in the AWS management console, you can have Kinesis Firehose configured to get data from Kinesis data stream. And put into a destination like Amazon S3, Redshift, Amazon Elastic Search, HTTP endpoints, or third-party service providers such as Datadog, Splunk, and others. No need to write any code for data consumption. So you cannot manage how you want to consume the data.
Firehose can transform the data to other formats like JSON, or can be compressed or secured data.
Here, you pay only for the data ingested, so inexpensive.

#### Amazon Data Analytics

If you want to control the code that is analyzing your data, the easiest and most popular way is via AWS Lambda functions. Since both are WS managing services, there is some polling made behind the curtains that invoked the lambda function when there is new data in the stream. So the lambda function that receives the data, receives the data without having to concern with the polling part. Looking a lot like a push-based architecture. Interacting with Kinesis via lambda is easier than hosting code on EC2, but still requires you to write the code that will run in the Lambda function.

You may want to choose an even more convenient way off analyzing that streaming data. If you want to go fully serverals without writing that code. You can rely on Kinesis Analytics, a powerful real time processing service with no servers to manage, and as usual, paying you only for what you use. Kinesis Analytics allow you to write SQL queries to process data in real time, providing an easy way to query streaming data without having to learn new frameworks or languages. You can also write Apache Flink code for more sophisticated analysis. 

Kinesis Analytics has two main concepts that are easy to understand. 
1. in-application streams
2. data pumps

#### Amazon Video Stream
Last but not least, you can also use Kinesis to ingest video and build video analytics applications. Kinesis Video Streams makes it easy to stream video from connected devices to AWS. You can use Kinesis video to make an application that would interact with a camera enabled doorbell from your mobile phone, for example.


### Batch Data Ingestion with AWS Services
One of the core capabilities of a data lake architecture is the ability to quickly and easily ingest multiple types of data, such as real-time streaming data and bulk data assets from on-premises storage platforms, as well as data generated and processed by legacy on-premises platforms, such as mainframes and data warehouses. AWS provides services and capabilities to cover all of these scenarios.

#### AWS Snow Family
Depending on where the data is, internet connectivity may not be reliable or the uplink may be used for something else, such as the production workload in a data center, not allowing you to copy large amounts of data using the Internet. AWS Snow Family, is a family of services designated for offline data transfer. 


### Data Cataloging
Now we're going to talk about a crucial aspect of data lakes, what 
to store in a data catalog and what are the AWS services used towards it. If you want to have a mature data architecture, you should keep track of all the assets as they were loaded into the data lake and then track all of the new data and virtues that were created by data transformation, processing and analytics. 

There are two general forms of a data catalog: a comprehensive data catalog that contains information about all assets that have been ingested into the S3 data lake, and a Hive Metastore Catalog (HCatalog) that contains information about data assets that have been transformed into formats and table definitions that are usable by analytics tools like Amazon Athena, Amazon Redshift, Amazon Redshift Spectrum, and Amazon EMR. The two catalogs are not mutually exclusive and both may exist. The comprehensive data catalog can be used to search for all assets in the data lake, and the HCatalog can be used to discover and query data assets in the data lake.

Think about the comprehensive catalog as being a general database containing everything. And the age catalog the tool used by processing services to extract insights. Most data lakes contains both age catalog and a comprehensive catalog.

**Comprehensive Data Catalog**

The comprehensive data catalog can be created by using standard AWS services like AWS Lambda, Amazon DynamoDB, and Amazon Elasticsearch Service (Amazon ES). At a high level, Lambda triggers are used to populate DynamoDB tables with object names and metadata when those objects are put into Amazon S3 then Amazon ES is used to search for specific assets, related metadata, and data classifications. The following figure shows a high-level architectural overview of this solution.

![]({{ site.url }}/assets/img/posts/comprehensive_data_catalog.png)

**HCatalog with AWS Glue**
AWS Glue can be used to create a Hive-compatible Metastore Catalog of data stored in an Amazon S3-based data lake. To use AWS Glue to build your data catalog, register your data sources with AWS Glue in the AWS Management Console. AWS Glue will then crawl your S3 buckets for data sources and construct a data catalog using pre-built classifiers for many popular source formats and data types, including JSON, CSV, Parquet, and more. You may also add your own classifiers or choose classifiers from the AWS Glue community to add to your crawls to recognize and catalog other data formats. The AWS Glue-generated catalog can be used by Amazon Athena, Amazon Redshift, Amazon Redshift Spectrum, and Amazon EMR, as well as third-party analytics tools that use a standard Hive Metastore Catalog. 

**Future Proofine the Data Lake**
A data lake built on AWS can immediately solve a broad range of business analytics challenges and quickly provide value to your business. However, business needs are constantly evolving, AWS and the analytics partner ecosystem are rapidly evolving and adding new services and capabilities, as businesses and their data lake users achieve more experience and analytics sophistication over time. Therefore, it’s important that the data lake can seamlessly and non-disruptively evolve as needed.

AWS futureproofs your data lake with a standardized storage solution that grows with your organization by ingesting and storing all of your business’s data assets on a platform with virtually unlimited scalability and well-defined APIs and integrates with a wide variety of data processing tools. This allows you to add new capabilities to your data lake as you need them without infrastructure limitations or barriers. Additionally, you can perform agile analytics experiments against data lake assets to quickly explore new processing methods and tools, and then scale the promising ones into production without the need to build new infrastructure, duplicate and/or migrate data, and have users migrate to a new platform. In closing, a data lake built on AWS allows you to evolve your business around your data assets, and to use these data assets to quickly and agilely drive more business value and competitive differentiation without limits.


### Data Preparation and AWS Glue Jobs
Ingesting raw data into a data lake is one thing. But actually making it useful is a totally different task that actually will account for probably 80 percent of the work. Data gets ingested in it's raw format, and as we know, that data may or may not be formatted correctly for our use case.

We'll likely spend a majority of our time writing scripts and devising solutions for making the data that you've already collected ready to be analyzed. It's even common to process the same data multiple times. We could use a dataset to generate reports for one use case, but then use that same dataset to train a machine learning model in a totally different use case. It's unlikely that these two use cases would require the data to be in the same format, which means processing the same data multiple times to fit each use case. 

A data lake can be seen as a central repository for data. Because of this, you cannot assume that you will be the only one using that data over time. Remember the idea of future-proofing? But yet, you might think that you're the only one using your data now, but others might find use for in the future. Therefore, leaving data in its raw format is a great idea. Others can prep the data the way that they see fit, starting from the raw source. 

- Since it's likely you'll need to process the data in your data lake at least one time, you want to make sure that you treat the original copy of the data as immutable, meaning that you cannot change it once it's been uploaded. You can make copies of the data, but the original data that was ingested remains untouched.
- You can use S3 lifecycle policies to move raw data to more cost-effective storage tiers as it becomes more infrequently accessed over time.
- You can use S3 features like object lock to ensure that no one can modify the raw data asset. Any changes done to the data would be made and then saved into another S3 location used specifically for prep data. Any changes done to the data would be made and then saved into another S3 location used specifically for prep data. The analytical tools can then read the data from the prep data location, not from the raw source location. Leaving the raw source untouched allows you to format it in different ways depending on who is using the data. 

There are multiple types of Dataprep that you might create process for.

1. Shaping
Some examples of shaping data is when you select only the fields you need to use for analysis, combining fields or files into one, or otherwise transforming and aggregating data in regards to its shape or schema. For example, you might find that the data is organically ingested across multiple files. But in order to analyze it efficiently, it makes sense to join the data across those files into one main file.
2. Blending
This is taking datasets that are originally ingested using different schemas or formats, and then changing or tweaking the datasets so that the schema or format matches in order to analyze it. 
3. Cleaning
This is when you are filling in missing values in a dataset, resolving any data conflicts or normalizing the data to common definitions. 

With data lakes, you'll likely be doing all types of Dataprep at one time or another. With the scale of data that we are talking about here, using automation to prep data is key.

you can use AWS Glue Jobs to automatically run scripts that transform data based on triggers or run on a schedule, or of course, you can use AWS Lambda functions that get triggered as data is uploaded to your a data lake. Just know that spending a lot of time setting up the processes for Dataprep is normal, and will account for a large percentage of the work involved with setting up your data lake. Lambda is best used for transformation of real-time data since you can trigger Lambdas to run as data comes in, while Glue Jobs is best used for processing data in batches.

AWS Glue has three main components.
1. Glue Data Catalog (see above)
2. Crawlers and the classifier (see above)
3. Glue Jobs

Let's focus on Glue Jobs. A job is the business logic that performs the ETL work in AWS Glue. This diagram shows the process of creating a job. See the numbers 1 through 6 here that are labeled, we're going to go over each one.

![]({{ site.url }}/assets/img/posts/aws_glue_job.png)

1. We must choose a data source for our job. The tables that represent your data source must already be defined in your data catalog. The data source is most likely the raw data before transformation.
2. Next, we choose a data target. This is where the data that gets transformed will be loaded. We can either designate a table from the data catalog to be the target or we can have the job create a new table when it runs. If we're using Amazon Athena, the tables can point to data sources like a DynamoDB table, an S3 bucket, or a database that requires a JDBC connection.
3. Then we provide customized configurations for our job, and then a PySpark script gets automatically generated for us. Examples of items you can figure for your job include, what type of job environment to run in. You could run your job as an Apache ETL script, a Spark streaming script, or a Python shell script. It depends on your use case, which one you would choose. You can choose to let Glue generate a script for you, or you can provide an S3 location with a script that you have already written for the job to use.
4. Another configuration you provide is the transformation type, which can change the schema, which allows you to change the schema of a source data and create a new target dataset, or you can choose to find matching records. You also configure logging and monitoring requirements. After you provide all of your configurations, AWS Glue generates a PySpark script and you can then edit the script to add things like transforms or whatever PySpark code that you want to add. Glue provides a set of built-in transforms that you can use to process your data. You can call these transforms from your ETL script. Some examples of built-in transforms Glue provides are things like dropping null fields, filtering records, joining datasets, mapping fields from source to target, and more. 
5. After you have edited the jobs you're liking, you then determine when you want the job to run, and configure a trigger or set up a schedule for the job to run. 
6. Finally, when all of this is done, you will have a script. In the AWS Glue console, descriptive is represented as code that you can both read and edit. Glue Jobs gives you a great starting point for beginners working with PySpark for the first time. It runs the script on essentially what is a managed Hadoop cluster. 


### File Optimizations
Columnar Data Format and Athena Partitioning

Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications.Columnar storage formats have the following characteristics that make them suitable for using with Athena:

Compression by column, with compression algorithm selected for the column data type to save storage space in Amazon S3 and reduce disk space and I/O during query processing.
Predicate pushdown in Parquet and ORC enables Athena queries to fetch only the blocks it needs, improving query performance. When an Athena query obtains specific column values from your data, it uses statistics from data block predicates, such as max/min values, to determine whether to read or skip the block.
Splitting of data in Parquet and ORC allows Athena to split the reading of data to multiple readers and increase parallelism during its query processing.

By partitioning your data, you can restrict the amount of data Athena scans by each query, thus improving performance and reducing cost. Athena leverages Hive for partitioning data. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. 

### Data Lake Securtiy
Let's focus on the security and compliance aspect of setting up your data lake. When using a data lake, you will likely have multiple teams using various technologies to analyze the data. 

People will have different roles and therefore will have different uses for the data lake, and will likely need different permissions. The data scientists needs programmatic access to S three directly so that they can run their custom processing jobs. Whereas the analyst wants to visualize a subset of the data using a tool like Kibana. Another use case could be a business person who just wants to be able to search the data lake for documents to view what sort of data is available. These are all different consumers of the data in your data lake. And you want to scope data access and permissions in a granular way that fits the use case for each individual role.
 
Cloud security at AWS is the highest priority. As an AWS customer, you benefit from a data center and network architecture that is built to meet the requirements of the most security-sensitive organizations.   Security is a shared responsibility between AWS and you. The shared responsibility model describes this as security of the cloud and security in the cloud:  

Security of the cloud – AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud. AWS also provides you with services that you can use securely. Third-party auditors regularly test and verify the effectiveness of our security as part of the AWS compliance programs. 
Security in the cloud – Your responsibility is determined by the AWS service that you use. You are also responsible for other factors including the sensitivity of your data, your company’s requirements, and applicable laws and regulations.


### Data Visualization with Amazon QuickSight   
Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. Amazon QuickSight connects to your data in the cloud and combines data from many different sources. In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more. As a fully managed cloud-based service, Amazon QuickSight provides enterprise-grade security, global availability, and built-in redundancy. It also provides the user-management tools that you need to scale from 10 users to 10,000, all with no infrastructure to deploy or manage.   

QuickSight gives decision-makers the opportunity to explore and interpret information in an interactive visual environment. They have secure access to dashboards from any device on your network and from mobile devices.   

When you import data into a dataset rather than using a direct SQL query, it becomes SPICE data because of how it's stored. SPICE is the Amazon QuickSight Super-fast, Parallel, In-memory Calculation Engine. It's engineered to rapidly perform advanced calculations and serve data. In Enterprise edition, data stored in SPICE is encrypted at rest.   

When you create or edit a dataset, you choose to use either SPICE or a direct query, unless the dataset contains uploaded files. Importing (also called ingesting) your data into SPICE can save time and money:  

Your analytical queries process faster.
You don't need to wait for a direct query to process.
Data stored in SPICE can be reused multiple times without incurring additional costs. If you use a data source that charges per query, you're charged for querying the data when you first create the dataset and later when you refresh the dataset.

Using Amazon QuickSight, you can access data and prepare it for use in reporting. It saves your prepared data either in SPICE memory or as a direct query. You can use a variety of data sources for analysis. When you create an analysis, the typical workflow looks like this:  

- Create a new analysis.
- Add new or existing datasets.
- Choose fields to create the first chart. QuickSight automatically suggests the best visualization.
- Add more charts, tables, or insights to the analysis. Resize and rearrange them on one or more sheets. Use extended features to add variables, custom controls, colors, additional pages (called sheets), and more.
- Publish the analysis as a dashboard to share it with other people.

**Registry of Open Data on AWS**   
When data is shared on AWS, anyone can analyze it and build services on top of it using a broad range of compute and data analytics products, including Amazon EC2, Amazon Athena, AWS Lambda, and Amazon EMR. Sharing data in the cloud lets data users spend more time on data analysis rather than data acquisition.   

The Registry of Open Data on AWS makes it easy to find datasets made publicly available through AWS services. Browse available data and learn how to register your own datasets.

